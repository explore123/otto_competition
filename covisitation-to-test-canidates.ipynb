{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e35ecf",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "## What is done in notebook:\n",
    "\n",
    "1. Generate covisitation matrices for test set, that is matrices that contains information about what items are \"clicked\" together. Code for covisitation matrices is forked and modified from Chris Deotte's covistiation matrices from [here](https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575). Additional matrice is added for carts and constants used in matrices generatation are adjusted.\n",
    "\n",
    "2. Generating 40 candidates for each session from test set, using covisitation matrices. While generating candidadates, \"weights of similarity\" between items are also saved, to be used as features in further models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc4462",
   "metadata": {
    "papermill": {
     "duration": 0.010541,
     "end_time": "2023-01-31T12:11:09.373967",
     "exception": false,
     "start_time": "2023-01-31T12:11:09.363426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f9e433",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:11:09.394128Z",
     "iopub.status.busy": "2023-01-31T12:11:09.393422Z",
     "iopub.status.idle": "2023-01-31T12:11:11.980167Z",
     "shell.execute_reply": "2023-01-31T12:11:11.979115Z"
    },
    "papermill": {
     "duration": 2.600177,
     "end_time": "2023-01-31T12:11:11.983632",
     "exception": false,
     "start_time": "2023-01-31T12:11:09.383455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "VER = 1\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "import os, sys, pickle, glob, gc\n",
    "import cudf, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import subprocess, psutil\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e4ee6",
   "metadata": {
    "papermill": {
     "duration": 0.008893,
     "end_time": "2023-01-31T12:11:14.563246",
     "exception": false,
     "start_time": "2023-01-31T12:11:14.554353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c2e42f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:11:14.583093Z",
     "iopub.status.busy": "2023-01-31T12:11:14.582294Z",
     "iopub.status.idle": "2023-01-31T12:11:14.587781Z",
     "shell.execute_reply": "2023-01-31T12:11:14.586769Z"
    },
    "papermill": {
     "duration": 0.017976,
     "end_time": "2023-01-31T12:11:14.590065",
     "exception": false,
     "start_time": "2023-01-31T12:11:14.572089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    local = False\n",
    "    track_gpu_memory = True\n",
    "    load_in_cache = False\n",
    "\n",
    "    print_steps = True\n",
    "    \n",
    "    keep_amount = 40\n",
    "    clip_amount_in_score_calc = 40\n",
    "    \n",
    "    weight_const1_carts = 0.1\n",
    "#     weight_const1_carts = 1.0\n",
    "    weight_const1_orders = 1.5\n",
    "    \n",
    "    add_most_common = False # very low benefit\n",
    "    \n",
    "    recreate_matrices = True\n",
    "    save_matrices = True\n",
    "    mat_keep_amount = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7aa6f",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba99563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:11:14.610031Z",
     "iopub.status.busy": "2023-01-31T12:11:14.609769Z",
     "iopub.status.idle": "2023-01-31T12:11:14.616121Z",
     "shell.execute_reply": "2023-01-31T12:11:14.615251Z"
    },
    "papermill": {
     "duration": 0.019136,
     "end_time": "2023-01-31T12:11:14.618136",
     "exception": false,
     "start_time": "2023-01-31T12:11:14.599000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "\n",
    "def read_file(f):\n",
    "    if cfg.load_in_cache:\n",
    "        return cudf.DataFrame( data_cache[f] )\n",
    "    else:\n",
    "        return read_file_to_cudf(f)\n",
    "        \n",
    "def read_file_to_cudf(f):\n",
    "    df = cudf.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32')\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df\n",
    "def read_file_to_cache(f):\n",
    "    df = pd.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32')\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887de718",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-01-31T12:11:14.638379Z",
     "iopub.status.busy": "2023-01-31T12:11:14.637055Z",
     "iopub.status.idle": "2023-01-31T12:11:14.673990Z",
     "shell.execute_reply": "2023-01-31T12:11:14.672926Z"
    },
    "papermill": {
     "duration": 0.049095,
     "end_time": "2023-01-31T12:11:14.676305",
     "exception": false,
     "start_time": "2023-01-31T12:11:14.627210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will process 146 files, in groups of 5 and chunks of 25.\n",
      "CPU times: user 1.03 ms, sys: 1.01 ms, total: 2.04 ms\n",
      "Wall time: 29 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n",
    "data_cache = {}\n",
    "\n",
    "if cfg.local:\n",
    "    files = glob.glob('../input/otto-validation/*_parquet/*')\n",
    "else:\n",
    "    files = glob.glob('../input/otto-chunk-data-inparquet-format/*_parquet/*')\n",
    "    test_files = glob.glob('../input/otto-chunk-data-inparquet-format/test_parquet/*')\n",
    "    \n",
    "if cfg.load_in_cache:\n",
    "    for f in files: data_cache[f] = read_file_to_cache(f)\n",
    "\n",
    "# CHUNK PARAMETERS\n",
    "READ_CT = 5\n",
    "DIVIDE_BY = 6\n",
    "CHUNK = int( np.ceil( len(files)/DIVIDE_BY ))\n",
    "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24e473d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:11:14.697106Z",
     "iopub.status.busy": "2023-01-31T12:11:14.695740Z",
     "iopub.status.idle": "2023-01-31T12:11:14.704701Z",
     "shell.execute_reply": "2023-01-31T12:11:14.703668Z"
    },
    "papermill": {
     "duration": 0.021176,
     "end_time": "2023-01-31T12:11:14.707177",
     "exception": false,
     "start_time": "2023-01-31T12:11:14.686001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "if cfg.recreate_matrices:\n",
    "    files_for_top_20_clicks = []\n",
    "    files_for_top_20_buys = []\n",
    "    files_for_top_20_buy2buy = []\n",
    "    files_for_click2cart = []\n",
    "else:\n",
    "    files_for_top_20_clicks = glob.glob('/kaggle/input/covisitation-to-test-candidates-dataset/top_20_clicks_*')\n",
    "    files_for_top_20_buys = glob.glob('/kaggle/input/covisitation-to-test-candidates-dataset/top_15_carts_orders_*')\n",
    "    files_for_top_20_buy2buy = glob.glob('/kaggle/input/covisitation-to-test-candidates-dataset/top_15_buy2buy_*')\n",
    "    files_for_click2cart = glob.glob('/kaggle/input/covisitation-to-test-candidates-dataset/top_click2cart_v*')\n",
    "\n",
    "print(len(files_for_top_20_clicks))\n",
    "print(len(files_for_top_20_buys))\n",
    "print(len(files_for_top_20_buy2buy))\n",
    "print(len(files_for_click2cart))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf23ed",
   "metadata": {
    "papermill": {
     "duration": 0.00913,
     "end_time": "2023-01-31T12:11:14.725452",
     "exception": false,
     "start_time": "2023-01-31T12:11:14.716322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e61e3676",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:11:14.746133Z",
     "iopub.status.busy": "2023-01-31T12:11:14.744586Z",
     "iopub.status.idle": "2023-01-31T12:11:14.751426Z",
     "shell.execute_reply": "2023-01-31T12:11:14.750582Z"
    },
    "papermill": {
     "duration": 0.019063,
     "end_time": "2023-01-31T12:11:14.753363",
     "exception": false,
     "start_time": "2023-01-31T12:11:14.734300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ts_format(ts):\n",
    "    return datetime.datetime.fromtimestamp(ts).strftime(\"%b %d %Y  %H:%M:%S\")\n",
    "def print_date_ts(ts, end='\\n'):\n",
    "    print(ts_format(ts), end=end)\n",
    "def print_date(d, end='\\n'):\n",
    "    print(d.strftime(\"%b %d %Y  %H:%M:%S\"), end=end)\n",
    "def mils_format(x):\n",
    "    return f'{x:,}'\n",
    "def len_mils(x):\n",
    "    return mils_format(len(x))\n",
    "def preview_df(df, head_show=1):\n",
    "    print(len_mils(df))\n",
    "    display(df.head(head_show))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d0e2ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:11:14.774101Z",
     "iopub.status.busy": "2023-01-31T12:11:14.772558Z",
     "iopub.status.idle": "2023-01-31T12:11:14.777197Z",
     "shell.execute_reply": "2023-01-31T12:11:14.776309Z"
    },
    "papermill": {
     "duration": 0.016527,
     "end_time": "2023-01-31T12:11:14.779168",
     "exception": false,
     "start_time": "2023-01-31T12:11:14.762641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc55f00",
   "metadata": {},
   "source": [
    "## Track memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_available = None\n",
    "\n",
    "try:\n",
    "    subprocess.check_output('nvidia-smi')\n",
    "    gpu_available = True\n",
    "\n",
    "    print('Nvidia GPU detected!')\n",
    "except Exception:\n",
    "    gpu_available = False\n",
    "    cfg.track_gpu_mem = False\n",
    "\n",
    "    print('No Nvidia GPU in system!')\n",
    "    \n",
    "if gpu_available:\n",
    "    dev0 = cudf.cupy.cuda.Device(0)\n",
    "    def print_gpu_mem_info():\n",
    "        free_memory, total_memory = dev0.mem_info\n",
    "        print('Free -', free_memory // 1024**2, 'MiB', '\\tTotal -', total_memory // 1024**2, 'MiB')\n",
    "    print_gpu_mem_info()\n",
    "    \n",
    "def clear_gpu_log():\n",
    "    with open('./gpu-report.csv', 'w') as file:\n",
    "        cols = [\n",
    "            'date', 'used_MiB'\n",
    "        ]\n",
    "        line = ','.join(cols) + '\\n'\n",
    "        file.write(line)\n",
    "\n",
    "def start_logging_gpu_info(every='2'):\n",
    "    log_gpu_subprocess = subprocess.Popen(\n",
    "        [\"watch\",\"-n\",every, \"./track-gpu.sh\"],\n",
    "        stdout=subprocess.PIPE\n",
    "    )\n",
    "    \n",
    "    return log_gpu_subprocess\n",
    "\n",
    "def stop_logging_gpu_info():\n",
    "    for proc in psutil.process_iter():\n",
    "        if 'watch' in proc.name() and './track-gpu.sh' in proc.cmdline():\n",
    "            proc.kill()\n",
    "\n",
    "def plot_gpu_usage(mins_to_plot=2, interval=2):\n",
    "    if not os.path.exists('./gpu-report.csv'):\n",
    "        return\n",
    "        \n",
    "    df = pd.read_csv('./gpu-report.csv')\n",
    "\n",
    "    entries_num = int(mins_to_plot * 60 // interval)\n",
    "    df = df[-entries_num:]\n",
    "    df['date'] = df.date.apply(lambda d: datetime.datetime.fromisoformat(d))\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.title(f'used GPU Memory')\n",
    "    plt.plot(df.date, df['used_MiB'], color='blue')\n",
    "    plt.show()\n",
    "\n",
    "def create_file_with_logging_command():\n",
    "    with open('./track-gpu.sh', 'w') as file:\n",
    "        file.write(\n",
    "            'values=$(nvidia-smi | sed -n \\'10p\\' | awk \\'{print $9}\\' | sed -nz \\'s/MiB\\\\n//gp\\'); echo $(date +%Y-%m-%d#%H:%M:%S),$(($values)) >> ' + f'{os.getcwd()}/gpu-report.csv'\n",
    "        )\n",
    "\n",
    "def run_logging_file():\n",
    "    subprocess.run(\n",
    "        [f'{os.getcwd()}/track-gpu.sh'],\n",
    "        shell=True\n",
    "    )\n",
    "\n",
    "def allow_logging_file_execution():\n",
    "    os.system('chmod a+x ./track-gpu.sh')\n",
    "    \n",
    "if cfg.track_gpu_mem:\n",
    "    create_file_with_logging_command()\n",
    "    allow_logging_file_execution()\n",
    "\n",
    "    stop_logging_gpu_info()\n",
    "    clear_gpu_log()\n",
    "    run_logging_file() # to add one memory value\n",
    "    \n",
    "    watch_process = start_logging_gpu_info()\n",
    "    \n",
    "    df = pd.read_csv('./gpu-report.csv')\n",
    "    df.tail()\n",
    "    \n",
    "if cfg.track_gpu_mem:\n",
    "    sleep(2)\n",
    "    plot_gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88630505",
   "metadata": {},
   "source": [
    "# Step 1 - Compute Co-visitation Matrices\n",
    "\n",
    "We will compute 4 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! For maximum speed, set the variable `DISK_PIECES` to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use `DISK_PIECES = 1` and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use `DISK_PIECES = 4` and it takes an amazing 3 minutes each! Below are some of the tricks to speed up computation\n",
    "* Use RAPIDS cuDF GPU instead of Pandas CPU\n",
    "* Read disk once and save in CPU RAM for later GPU multiple use\n",
    "* Process largest amount of data possible on GPU at one time\n",
    "* Merge data in two stages. Multiple small to single medium. Multiple medium to single large.\n",
    "* Write result as parquet instead of dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb0f60",
   "metadata": {
    "papermill": {
     "duration": 0.008813,
     "end_time": "2023-01-31T12:11:14.797091",
     "exception": false,
     "start_time": "2023-01-31T12:11:14.788278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f57652",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:11:14.816775Z",
     "iopub.status.busy": "2023-01-31T12:11:14.816522Z",
     "iopub.status.idle": "2023-01-31T12:16:25.574327Z",
     "shell.execute_reply": "2023-01-31T12:16:25.573301Z"
    },
    "papermill": {
     "duration": 310.78629,
     "end_time": "2023-01-31T12:16:25.592450",
     "exception": false,
     "start_time": "2023-01-31T12:11:14.806160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  \"When using a sequence of booleans for `ascending`, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5... 25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5... 50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5... 75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5... 100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5... 125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5... 0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5... 25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5... 50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5... 75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5... 100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5... 125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5... 0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5... 25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5... 50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5... 75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5... 100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5... 125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 24 in groups of 5... 0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5... 25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5... 50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5... 75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5... 100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5... 125 , 130 , 135 , 140 , 145 , \n",
      "['top_15_carts_orders_v6_0.pqt', 'top_15_carts_orders_v6_2.pqt', 'top_15_carts_orders_v6_1.pqt', 'top_15_carts_orders_v6_3.pqt']\n",
      "CPU times: user 3min 21s, sys: 1min 11s, total: 4min 33s\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# files_for_top_20_buys = []\n",
    "if len(files_for_top_20_buys) == 0:\n",
    "    type_weight = {0:1, 1:6, 2:3}\n",
    "\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for PART in range(DISK_PIECES):\n",
    "        print()\n",
    "        print('### DISK PART',PART+1)\n",
    "\n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(DIVIDE_BY):\n",
    "            a = j*CHUNK\n",
    "            b = min( (j+1)*CHUNK, len(files) )\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...', end=' ')\n",
    "\n",
    "            # => INNER CHUNKS\n",
    "            for k in range(a,b,READ_CT):\n",
    "                # READ FILE\n",
    "                df = [read_file(files[k])]\n",
    "                for i in range(1,READ_CT): \n",
    "                    if k+i<b: df.append( read_file(files[k+i]) )\n",
    "                df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "                df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df,on='session')\n",
    "                df = df.loc[ ((df.ts_x - df.ts_y).abs()< 30 * 60) & (df.aid_x != df.aid_y) ]\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df['wgt'] = df.type_y.map(type_weight)\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a: tmp2 = df\n",
    "                else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "                print(k,', ',end='')\n",
    "            print()\n",
    "            # COMBINE OUTER CHUNKS\n",
    "            if a==0: tmp = tmp2\n",
    "            else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 40\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n < cfg.mat_keep_amount].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "\n",
    "        tmp = tmp.to_pandas()\n",
    "        tmp.wgt = np.log(1.73+tmp.wgt)\n",
    "        tmp.to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')\n",
    "\n",
    "        files_for_top_20_buys = glob.glob('top_15_carts_orders_v*')\n",
    "\n",
    "        # tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')\n",
    "    del tmp\n",
    "print(files_for_top_20_buys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "419cd0ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:16:25.625374Z",
     "iopub.status.busy": "2023-01-31T12:16:25.624559Z",
     "iopub.status.idle": "2023-01-31T12:16:29.013578Z",
     "shell.execute_reply": "2023-01-31T12:16:29.012148Z"
    },
    "papermill": {
     "duration": 3.41003,
     "end_time": "2023-01-31T12:16:29.018069",
     "exception": false,
     "start_time": "2023-01-31T12:16:25.608039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_15_carts_orders_v6_0.pqt\n",
      "top_15_carts_orders_v6_2.pqt\n",
      "top_15_carts_orders_v6_1.pqt\n",
      "top_15_carts_orders_v6_3.pqt\n"
     ]
    }
   ],
   "source": [
    "if cfg.save_matrices:\n",
    "    for f in files_for_top_20_buys:\n",
    "        tmp = cudf.read_parquet(f)\n",
    "        file_name = os.path.basename(f)\n",
    "        print(file_name)\n",
    "        tmp.to_parquet(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519cffe",
   "metadata": {
    "papermill": {
     "duration": 0.015638,
     "end_time": "2023-01-31T12:16:29.062157",
     "exception": false,
     "start_time": "2023-01-31T12:16:29.046519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2) \"Buy2Buy\" Co-visitation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed6d0039",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-01-31T12:16:29.095202Z",
     "iopub.status.busy": "2023-01-31T12:16:29.094867Z",
     "iopub.status.idle": "2023-01-31T12:17:23.470787Z",
     "shell.execute_reply": "2023-01-31T12:17:23.469345Z"
    },
    "papermill": {
     "duration": 54.395175,
     "end_time": "2023-01-31T12:17:23.472980",
     "exception": false,
     "start_time": "2023-01-31T12:16:29.077805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will process 146 files, in groups of 5 and chunks of 30.\n",
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 29 in groups of 5...\n",
      "0 1 2 3 4 5 6 7 8 9 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  \"When using a sequence of booleans for `ascending`, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
      "Processing files 30 thru 59 in groups of 5...\n",
      "30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n",
      "Processing files 60 thru 89 in groups of 5...\n",
      "60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 \n",
      "Processing files 90 thru 119 in groups of 5...\n",
      "90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 \n",
      "Processing files 120 thru 145 in groups of 5...\n",
      "120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 \n",
      "top_15_buy2buy_v6_0.pqt\n",
      "21,489,751\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aid_x</th>\n",
       "      <th>aid_y</th>\n",
       "      <th>wgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>279811</td>\n",
       "      <td>1.004302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aid_x   aid_y       wgt\n",
       "0      1  279811  1.004302"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['top_15_buy2buy_v6_0.pqt']\n",
      "CPU times: user 42.5 s, sys: 10.5 s, total: 53.1 s\n",
      "Wall time: 54.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# files_for_top_20_buy2buy = []\n",
    "if len(files_for_top_20_buy2buy) == 0:\n",
    "\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    DISK_PIECES = 1\n",
    "    SIZE = 1.86e6/DISK_PIECES\n",
    "    DIVIDE_BY = 5\n",
    "    READ_CT = 5\n",
    "    CHUNK = int( np.ceil( len(files)/DIVIDE_BY ))\n",
    "    print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')\n",
    "\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for PART in range(DISK_PIECES):\n",
    "        print()\n",
    "        print('### DISK PART',PART+1)\n",
    "\n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(DIVIDE_BY):\n",
    "            a = j*CHUNK\n",
    "            b = min( (j+1)*CHUNK, len(files) )\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "\n",
    "            # => INNER CHUNKS\n",
    "            for k in range(a,b,READ_CT):\n",
    "                # READ FILE\n",
    "                df = [read_file(files[k])]\n",
    "                print(k, end=' ')\n",
    "                for i in range(1,READ_CT): \n",
    "                    if k+i<b:\n",
    "                        print(k+i, end=' ')\n",
    "                        df.append( read_file(files[k+i]) )\n",
    "\n",
    "                df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "                df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
    "                df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<20].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df,on='session')\n",
    "                df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "\n",
    "                # df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                # df['wgt'] = 1 + 3*(df.ts_x - ts_min_used)/(ts_max_used-ts_min_used)\n",
    "\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df['wgt'] = 1\n",
    "\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a: tmp2 = df\n",
    "                else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "                # print(k,', ',end='')\n",
    "            print()\n",
    "            # COMBINE OUTER CHUNKS\n",
    "            if a==0: tmp = tmp2\n",
    "            else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 40\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n < cfg.mat_keep_amount].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "\n",
    "        tmp = tmp.to_pandas()\n",
    "        tmp.wgt = np.log(1.73+tmp.wgt)\n",
    "        tmp.to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')\n",
    "\n",
    "        # tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')\n",
    "    del tmp\n",
    "    files_for_top_20_buy2buy = glob.glob(f'top_15_buy2buy_v{VER}*')\n",
    "\n",
    "    # Carts and orders occuring together:\n",
    "\n",
    "    top_20_buy2buy_df = cudf.DataFrame()\n",
    "    for f in files_for_top_20_buy2buy:\n",
    "        print(f)\n",
    "        top_20_buy2buy_df = cudf.concat([top_20_buy2buy_df, cudf.read_parquet(f)])\n",
    "    preview_df(top_20_buy2buy_df)\n",
    "    \n",
    "print(files_for_top_20_buy2buy)\n",
    "\n",
    "# 1,314,327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ada1a2a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:17:23.515754Z",
     "iopub.status.busy": "2023-01-31T12:17:23.515469Z",
     "iopub.status.idle": "2023-01-31T12:17:24.502996Z",
     "shell.execute_reply": "2023-01-31T12:17:24.501431Z"
    },
    "papermill": {
     "duration": 1.012986,
     "end_time": "2023-01-31T12:17:24.507137",
     "exception": false,
     "start_time": "2023-01-31T12:17:23.494151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_15_buy2buy_v6_0.pqt\n"
     ]
    }
   ],
   "source": [
    "if cfg.save_matrices:\n",
    "    for f in files_for_top_20_buy2buy:\n",
    "        tmp = cudf.read_parquet(f)\n",
    "        file_name = os.path.basename(f)\n",
    "        print(file_name)\n",
    "        tmp.to_parquet(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fbc33b",
   "metadata": {
    "papermill": {
     "duration": 0.020206,
     "end_time": "2023-01-31T12:17:24.552770",
     "exception": false,
     "start_time": "2023-01-31T12:17:24.532564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3) \"Clicks\" Co-visitation Matrix - Time Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "359d7043",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-01-31T12:17:24.595151Z",
     "iopub.status.busy": "2023-01-31T12:17:24.594768Z",
     "iopub.status.idle": "2023-01-31T12:22:01.401339Z",
     "shell.execute_reply": "2023-01-31T12:22:01.400167Z"
    },
    "papermill": {
     "duration": 276.873908,
     "end_time": "2023-01-31T12:22:01.446877",
     "exception": false,
     "start_time": "2023-01-31T12:17:24.572969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 29 in groups of 5... 0 , 1 2 3 4 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  \"When using a sequence of booleans for `ascending`, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 , 6 7 8 9 10 , 11 12 13 14 15 , 16 17 18 19 20 , 21 22 23 24 25 , 26 27 28 29 \n",
      "Processing files 30 thru 59 in groups of 5... 30 , 31 32 33 34 35 , 36 37 38 39 40 , 41 42 43 44 45 , 46 47 48 49 50 , 51 52 53 54 55 , 56 57 58 59 \n",
      "Processing files 60 thru 89 in groups of 5... 60 , 61 62 63 64 65 , 66 67 68 69 70 , 71 72 73 74 75 , 76 77 78 79 80 , 81 82 83 84 85 , 86 87 88 89 \n",
      "Processing files 90 thru 119 in groups of 5... 90 , 91 92 93 94 95 , 96 97 98 99 100 , 101 102 103 104 105 , 106 107 108 109 110 , 111 112 113 114 115 , 116 117 118 119 \n",
      "Processing files 120 thru 145 in groups of 5... 120 , 121 122 123 124 125 , 126 127 128 129 130 , 131 132 133 134 135 , 136 137 138 139 140 , 141 142 143 144 145 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 29 in groups of 5... 0 , 1 2 3 4 5 , 6 7 8 9 10 , 11 12 13 14 15 , 16 17 18 19 20 , 21 22 23 24 25 , 26 27 28 29 \n",
      "Processing files 30 thru 59 in groups of 5... 30 , 31 32 33 34 35 , 36 37 38 39 40 , 41 42 43 44 45 , 46 47 48 49 50 , 51 52 53 54 55 , 56 57 58 59 \n",
      "Processing files 60 thru 89 in groups of 5... 60 , 61 62 63 64 65 , 66 67 68 69 70 , 71 72 73 74 75 , 76 77 78 79 80 , 81 82 83 84 85 , 86 87 88 89 \n",
      "Processing files 90 thru 119 in groups of 5... 90 , 91 92 93 94 95 , 96 97 98 99 100 , 101 102 103 104 105 , 106 107 108 109 110 , 111 112 113 114 115 , 116 117 118 119 \n",
      "Processing files 120 thru 145 in groups of 5... 120 , 121 122 123 124 125 , 126 127 128 129 130 , 131 132 133 134 135 , 136 137 138 139 140 , 141 142 143 144 145 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 29 in groups of 5... 0 , 1 2 3 4 5 , 6 7 8 9 10 , 11 12 13 14 15 , 16 17 18 19 20 , 21 22 23 24 25 , 26 27 28 29 \n",
      "Processing files 30 thru 59 in groups of 5... 30 , 31 32 33 34 35 , 36 37 38 39 40 , 41 42 43 44 45 , 46 47 48 49 50 , 51 52 53 54 55 , 56 57 58 59 \n",
      "Processing files 60 thru 89 in groups of 5... 60 , 61 62 63 64 65 , 66 67 68 69 70 , 71 72 73 74 75 , 76 77 78 79 80 , 81 82 83 84 85 , 86 87 88 89 \n",
      "Processing files 90 thru 119 in groups of 5... 90 , 91 92 93 94 95 , 96 97 98 99 100 , 101 102 103 104 105 , 106 107 108 109 110 , 111 112 113 114 115 , 116 117 118 119 \n",
      "Processing files 120 thru 145 in groups of 5... 120 , 121 122 123 124 125 , 126 127 128 129 130 , 131 132 133 134 135 , 136 137 138 139 140 , 141 142 143 144 145 , \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 29 in groups of 5... 0 , 1 2 3 4 5 , 6 7 8 9 10 , 11 12 13 14 15 , 16 17 18 19 20 , 21 22 23 24 25 , 26 27 28 29 \n",
      "Processing files 30 thru 59 in groups of 5... 30 , 31 32 33 34 35 , 36 37 38 39 40 , 41 42 43 44 45 , 46 47 48 49 50 , 51 52 53 54 55 , 56 57 58 59 \n",
      "Processing files 60 thru 89 in groups of 5... 60 , 61 62 63 64 65 , 66 67 68 69 70 , 71 72 73 74 75 , 76 77 78 79 80 , 81 82 83 84 85 , 86 87 88 89 \n",
      "Processing files 90 thru 119 in groups of 5... 90 , 91 92 93 94 95 , 96 97 98 99 100 , 101 102 103 104 105 , 106 107 108 109 110 , 111 112 113 114 115 , 116 117 118 119 \n",
      "Processing files 120 thru 145 in groups of 5... 120 , 121 122 123 124 125 , 126 127 128 129 130 , 131 132 133 134 135 , 136 137 138 139 140 , 141 142 143 144 145 , \n",
      "['top_20_clicks_v6_0.pqt', 'top_20_clicks_v6_2.pqt', 'top_20_clicks_v6_3.pqt', 'top_20_clicks_v6_1.pqt']\n",
      "CPU times: user 3min 21s, sys: 1min 10s, total: 4min 31s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# files_for_top_20_clicks = []\n",
    "if len(files_for_top_20_clicks) == 0:\n",
    "\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for PART in range(DISK_PIECES):\n",
    "        print()\n",
    "        print('### DISK PART',PART+1)\n",
    "\n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(DIVIDE_BY):\n",
    "            a = j*CHUNK\n",
    "            b = min( (j+1)*CHUNK, len(files) )\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...', end=' ')\n",
    "\n",
    "            # => INNER CHUNKS\n",
    "            for k in range(a,b,READ_CT):\n",
    "                print(k,', ',end='')\n",
    "                # READ FILE\n",
    "                df = [read_file(files[k])]\n",
    "                for i in range(1,READ_CT): \n",
    "                    if k+i<b: \n",
    "                        print(k+i, end=' ')\n",
    "                        df.append( read_file(files[k+i]) )\n",
    "                        \n",
    "                df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "                df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df,on='session')\n",
    "                df = df.loc[ ((df.ts_x - df.ts_y).abs()< 30 * 60) & (df.aid_x != df.aid_y) ]\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a: tmp2 = df\n",
    "                else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "#                 print(k,', ',end='')\n",
    "            print()\n",
    "            # COMBINE OUTER CHUNKS\n",
    "            if a==0: tmp = tmp2\n",
    "            else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 40\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n < cfg.mat_keep_amount].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "\n",
    "        tmp = tmp.to_pandas()\n",
    "        tmp.wgt = np.log(1.73+tmp.wgt)\n",
    "        tmp.to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')\n",
    "\n",
    "        # tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')\n",
    "        \n",
    "        files_for_top_20_clicks = glob.glob(f'top_20_clicks_v{VER}_*')\n",
    "    del tmp\n",
    "print(files_for_top_20_clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf750956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:22:01.561914Z",
     "iopub.status.busy": "2023-01-31T12:22:01.561546Z",
     "iopub.status.idle": "2023-01-31T12:22:04.818992Z",
     "shell.execute_reply": "2023-01-31T12:22:04.817415Z"
    },
    "papermill": {
     "duration": 3.303783,
     "end_time": "2023-01-31T12:22:04.823392",
     "exception": false,
     "start_time": "2023-01-31T12:22:01.519609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_20_clicks_v6_0.pqt\n",
      "top_20_clicks_v6_2.pqt\n",
      "top_20_clicks_v6_3.pqt\n",
      "top_20_clicks_v6_1.pqt\n"
     ]
    }
   ],
   "source": [
    "if cfg.save_matrices:\n",
    "    for f in files_for_top_20_clicks:\n",
    "        tmp = cudf.read_parquet(f)\n",
    "        file_name = os.path.basename(f)\n",
    "        print(file_name)\n",
    "        tmp.to_parquet(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807b791",
   "metadata": {
    "papermill": {
     "duration": 0.039488,
     "end_time": "2023-01-31T12:22:04.911877",
     "exception": false,
     "start_time": "2023-01-31T12:22:04.872389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4) Carts mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa1d2143",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:22:04.993890Z",
     "iopub.status.busy": "2023-01-31T12:22:04.993254Z",
     "iopub.status.idle": "2023-01-31T12:27:25.688613Z",
     "shell.execute_reply": "2023-01-31T12:27:25.687699Z"
    },
    "papermill": {
     "duration": 320.73963,
     "end_time": "2023-01-31T12:27:25.691140",
     "exception": false,
     "start_time": "2023-01-31T12:22:04.951510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 29 in groups of 4... 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \n",
      "Processing files 30 thru 59 in groups of 4... 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 \n",
      "Processing files 60 thru 89 in groups of 4... 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 \n",
      "Processing files 90 thru 119 in groups of 4... 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 \n",
      "Processing files 120 thru 145 in groups of 4... 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 29 in groups of 4... 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \n",
      "Processing files 30 thru 59 in groups of 4... 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 \n",
      "Processing files 60 thru 89 in groups of 4... 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 \n",
      "Processing files 90 thru 119 in groups of 4... 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 \n",
      "Processing files 120 thru 145 in groups of 4... 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 29 in groups of 4... 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \n",
      "Processing files 30 thru 59 in groups of 4... 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 \n",
      "Processing files 60 thru 89 in groups of 4... 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 \n",
      "Processing files 90 thru 119 in groups of 4... 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 \n",
      "Processing files 120 thru 145 in groups of 4... 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 29 in groups of 4... 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \n",
      "Processing files 30 thru 59 in groups of 4... 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 \n",
      "Processing files 60 thru 89 in groups of 4... 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 \n",
      "Processing files 90 thru 119 in groups of 4... 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 \n",
      "Processing files 120 thru 145 in groups of 4... 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 \n",
      "top_click2cart_v6_0.pqt\n",
      "top_click2cart_v6_3.pqt\n",
      "top_click2cart_v6_2.pqt\n",
      "top_click2cart_v6_1.pqt\n",
      "56,790,872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aid_x</th>\n",
       "      <th>aid_y</th>\n",
       "      <th>wgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>532042</td>\n",
       "      <td>2.275214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aid_x   aid_y       wgt\n",
       "0      0  532042  2.275214"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['top_click2cart_v6_0.pqt', 'top_click2cart_v6_3.pqt', 'top_click2cart_v6_2.pqt', 'top_click2cart_v6_1.pqt']\n",
      "CPU times: user 3min 52s, sys: 1min 22s, total: 5min 14s\n",
      "Wall time: 5min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# files_for_click2cart = []\n",
    "if len(files_for_click2cart) == 0:\n",
    "\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6/DISK_PIECES\n",
    "    READ_CT = 4\n",
    "    DIVIDE_BY = 5\n",
    "\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for PART in range(DISK_PIECES):\n",
    "        print()\n",
    "        print('### DISK PART',PART+1)\n",
    "\n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(DIVIDE_BY):\n",
    "            a = j*CHUNK\n",
    "            b = min( (j+1)*CHUNK, len(files) )\n",
    "            print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...', end=' ')\n",
    "\n",
    "            # => INNER CHUNKS\n",
    "            for k in range(a,b,READ_CT):\n",
    "                # READ FILE\n",
    "                df = [read_file(files[k])]\n",
    "                print(k, end=' ')\n",
    "                for i in range(1,READ_CT): \n",
    "                    print(k+i, end=' ')\n",
    "                    if k+i<b: df.append( read_file(files[k+i]) )\n",
    "                df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "\n",
    "                df = df.loc[df['type'] < 2] # CLICKS and CARTS \n",
    "                df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<20].drop('n',axis=1)\n",
    "\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df,on='session')\n",
    "                df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
    "\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "\n",
    "                # ASSIGN WEIGHTS\n",
    "\n",
    "                # df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                # df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df['wgt'] = 1\n",
    "                \n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                \n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a: tmp2 = df\n",
    "                else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "                # print(k,'-',k+i,', ',end='')\n",
    "            print()\n",
    "            # COMBINE OUTER CHUNKS\n",
    "            if a==0: tmp = tmp2\n",
    "            else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 40\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n < cfg.mat_keep_amount].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        \n",
    "        tmp = tmp.to_pandas()\n",
    "        tmp.wgt = np.log(1.73+tmp.wgt)\n",
    "        tmp.to_parquet(f'top_click2cart_v{VER}_{PART}.pqt')\n",
    "\n",
    "        # tmp.to_pandas().to_parquet(f'top_click2cart_v{VER}_{PART}.pqt')\n",
    "    del tmp\n",
    "    files_for_click2cart = glob.glob('top_click2cart_v*')\n",
    "\n",
    "    # Carts and clicks occuring together:\n",
    "\n",
    "    top_click2cart_df = cudf.DataFrame()\n",
    "    for f in files_for_click2cart:\n",
    "        print(f)\n",
    "        top_click2cart_df = cudf.concat([top_click2cart_df, cudf.read_parquet(f)])\n",
    "    preview_df(top_click2cart_df)\n",
    "\n",
    "print(files_for_click2cart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec207bd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:27:25.869697Z",
     "iopub.status.busy": "2023-01-31T12:27:25.869241Z",
     "iopub.status.idle": "2023-01-31T12:27:29.045061Z",
     "shell.execute_reply": "2023-01-31T12:27:29.043770Z"
    },
    "papermill": {
     "duration": 3.26977,
     "end_time": "2023-01-31T12:27:29.049458",
     "exception": false,
     "start_time": "2023-01-31T12:27:25.779688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_click2cart_v6_0.pqt\n",
      "top_click2cart_v6_3.pqt\n",
      "top_click2cart_v6_2.pqt\n",
      "top_click2cart_v6_1.pqt\n"
     ]
    }
   ],
   "source": [
    "if cfg.save_matrices:\n",
    "    for f in files_for_click2cart:\n",
    "        tmp = cudf.read_parquet(f)\n",
    "        file_name = os.path.basename(f)\n",
    "        print(file_name)\n",
    "        tmp.to_parquet(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71c959cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:27:29.177312Z",
     "iopub.status.busy": "2023-01-31T12:27:29.176947Z",
     "iopub.status.idle": "2023-01-31T12:27:29.321014Z",
     "shell.execute_reply": "2023-01-31T12:27:29.319842Z"
    },
    "papermill": {
     "duration": 0.205362,
     "end_time": "2023-01-31T12:27:29.323795",
     "exception": false,
     "start_time": "2023-01-31T12:27:29.118433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FREE MEMORY\n",
    "try:\n",
    "    if cfg.load_in_cache:\n",
    "        del data_cache\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beaf1e1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:27:29.441041Z",
     "iopub.status.busy": "2023-01-31T12:27:29.440693Z",
     "iopub.status.idle": "2023-01-31T12:27:29.446240Z",
     "shell.execute_reply": "2023-01-31T12:27:29.445366Z"
    },
    "papermill": {
     "duration": 0.066218,
     "end_time": "2023-01-31T12:27:29.448355",
     "exception": false,
     "start_time": "2023-01-31T12:27:29.382137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1676e7",
   "metadata": {
    "papermill": {
     "duration": 0.057334,
     "end_time": "2023-01-31T12:27:29.562904",
     "exception": false,
     "start_time": "2023-01-31T12:27:29.505570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 2 - ReRank - choose candidates using handcrafted rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87f18be6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:27:29.696884Z",
     "iopub.status.busy": "2023-01-31T12:27:29.696528Z",
     "iopub.status.idle": "2023-01-31T12:27:29.700839Z",
     "shell.execute_reply": "2023-01-31T12:27:29.699788Z"
    },
    "papermill": {
     "duration": 0.074965,
     "end_time": "2023-01-31T12:27:29.703190",
     "exception": false,
     "start_time": "2023-01-31T12:27:29.628225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "091662a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:27:29.826237Z",
     "iopub.status.busy": "2023-01-31T12:27:29.825320Z",
     "iopub.status.idle": "2023-01-31T12:27:31.493075Z",
     "shell.execute_reply": "2023-01-31T12:27:31.491383Z"
    },
    "papermill": {
     "duration": 1.731532,
     "end_time": "2023-01-31T12:27:31.495517",
     "exception": false,
     "start_time": "2023-01-31T12:27:29.763985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data has shape (6928123, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>aid</th>\n",
       "      <th>ts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13099779</td>\n",
       "      <td>245308</td>\n",
       "      <td>1661795832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13099779</td>\n",
       "      <td>245308</td>\n",
       "      <td>1661795862</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13099779</td>\n",
       "      <td>972319</td>\n",
       "      <td>1661795888</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13099779</td>\n",
       "      <td>972319</td>\n",
       "      <td>1661795898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13099779</td>\n",
       "      <td>245308</td>\n",
       "      <td>1661795907</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session     aid          ts  type\n",
       "0  13099779  245308  1661795832     0\n",
       "1  13099779  245308  1661795862     1\n",
       "2  13099779  972319  1661795888     0\n",
       "3  13099779  972319  1661795898     1\n",
       "4  13099779  245308  1661795907     0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_test():    \n",
    "    dfs = []\n",
    "    for e, chunk_file in enumerate(test_files):\n",
    "        chunk = pd.read_parquet(chunk_file)\n",
    "        chunk.ts = (chunk.ts/1000).astype('int32')\n",
    "        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n",
    "        dfs.append(chunk)\n",
    "    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n",
    "\n",
    "test_df = load_test()\n",
    "print('Test data has shape',test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "251267dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:27:31.628579Z",
     "iopub.status.busy": "2023-01-31T12:27:31.628231Z",
     "iopub.status.idle": "2023-01-31T12:27:32.095623Z",
     "shell.execute_reply": "2023-01-31T12:27:32.094717Z"
    },
    "papermill": {
     "duration": 0.543059,
     "end_time": "2023-01-31T12:27:32.098272",
     "exception": false,
     "start_time": "2023-01-31T12:27:31.555213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TOP CLICKS AND ORDERS IN TEST\n",
    "\n",
    "top_clicks = test_df.loc[test_df['type']==0,'aid'].value_counts().index.values[:cfg.keep_amount]\n",
    "top_carts = test_df.loc[test_df['type']==1,'aid'].value_counts().index.values[:cfg.keep_amount]\n",
    "top_orders = test_df.loc[test_df['type']==2,'aid'].value_counts().index.values[:cfg.keep_amount]\n",
    "\n",
    "top_clicks_df = pd.DataFrame({'aid':top_clicks})\n",
    "top_carts_df = pd.DataFrame({'aid':top_carts})\n",
    "top_orders_df = pd.DataFrame({'aid':top_orders})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0164ce3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:27:32.223907Z",
     "iopub.status.busy": "2023-01-31T12:27:32.223574Z",
     "iopub.status.idle": "2023-01-31T12:27:32.233746Z",
     "shell.execute_reply": "2023-01-31T12:27:32.232254Z"
    },
    "papermill": {
     "duration": 0.073005,
     "end_time": "2023-01-31T12:27:32.235849",
     "exception": false,
     "start_time": "2023-01-31T12:27:32.162844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_top_20_clicks_df():\n",
    "    top_20_clicks_df = cudf.DataFrame()\n",
    "    files_for_top_20_clicks.sort()\n",
    "    for f in files_for_top_20_clicks:\n",
    "        print(f, end=' ')\n",
    "        top_20_clicks_df = cudf.concat([top_20_clicks_df, cudf.read_parquet(f)])\n",
    "    # preview_df(top_20_clicks_df)\n",
    "\n",
    "    return top_20_clicks_df\n",
    "def load_top_20_buys_df():\n",
    "    top_20_buys_df = cudf.DataFrame()\n",
    "    for f in files_for_top_20_buys:\n",
    "        print(f, end=' ')\n",
    "        top_20_buys_df = cudf.concat([top_20_buys_df, cudf.read_parquet(f)])\n",
    "    # print()\n",
    "    # preview_df(top_20_buys_df)\n",
    "\n",
    "    return top_20_buys_df\n",
    "def load_top_20_buy2buy_df():\n",
    "    # Carts and orders occuring together:\n",
    "    top_20_buy2buy_df = cudf.DataFrame()\n",
    "    for f in files_for_top_20_buy2buy:\n",
    "        print(f, end=' ')\n",
    "        top_20_buy2buy_df = cudf.concat([top_20_buy2buy_df, cudf.read_parquet(f)])\n",
    "    # preview_df(top_20_buy2buy_df)\n",
    "\n",
    "    return top_20_buy2buy_df\n",
    "def load_top_click2cart_df():\n",
    "    # Carts and clicks occuring together:\n",
    "    top_click2cart_df = cudf.DataFrame()\n",
    "    for f in files_for_click2cart:\n",
    "        print(f, end=' ')\n",
    "        top_click2cart_df = cudf.concat([top_click2cart_df, cudf.read_parquet(f)])\n",
    "    # preview_df(top_click2cart_df)\n",
    "    return top_click2cart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "613dc293",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:27:32.360102Z",
     "iopub.status.busy": "2023-01-31T12:27:32.359222Z",
     "iopub.status.idle": "2023-01-31T12:27:32.369373Z",
     "shell.execute_reply": "2023-01-31T12:27:32.368467Z"
    },
    "papermill": {
     "duration": 0.076845,
     "end_time": "2023-01-31T12:27:32.371613",
     "exception": false,
     "start_time": "2023-01-31T12:27:32.294768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_event_recall(ev_preds_df, event_name, clip_amount=cfg.clip_amount_in_score_calc):\n",
    "    weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "    sub = ev_preds_df\n",
    "    sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "    sub.labels = sub.labels.apply(lambda x: x[:clip_amount])\n",
    "    \n",
    "    test_labels = pd.read_parquet(test_labels_file)\n",
    "    test_labels = test_labels.loc[test_labels['type']==event_name]\n",
    "    test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "\n",
    "    test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "    \n",
    "    test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0, clip_amount)\n",
    "    \n",
    "    recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "    # score += weights[event_name]*recall\n",
    "    print(f'{event_name} recall =',recall)\n",
    "\n",
    "    return recall\n",
    "    \n",
    "def calc_score(event_name, clip_amount):\n",
    "    ev_preds_df = {}\n",
    "    for ev in [event_name]:\n",
    "        files_for_ev = glob.glob(f'{ev}_pred_df*')\n",
    "        # print(ev, len(files_for_ev))\n",
    "        ev_preds_df[ev] = pd.DataFrame()\n",
    "        for f in files_for_ev:\n",
    "            ev_preds_df[ev] = pd.concat([ev_preds_df[ev], pd.read_parquet(f)], ignore_index=True)\n",
    "        # preview_df(ev_preds_df[ev])\n",
    "\n",
    "    return calc_event_recall(ev_preds_df[event_name], event_name, clip_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2180fa70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:27:32.492523Z",
     "iopub.status.busy": "2023-01-31T12:27:32.490839Z",
     "iopub.status.idle": "2023-01-31T12:27:32.499927Z",
     "shell.execute_reply": "2023-01-31T12:27:32.499073Z"
    },
    "papermill": {
     "duration": 0.071445,
     "end_time": "2023-01-31T12:27:32.501931",
     "exception": false,
     "start_time": "2023-01-31T12:27:32.430486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_most_common(df, most_common, event_name, keep_amount=cfg.keep_amount):\n",
    "    df = df.to_pandas()\n",
    "    tmp1 = df[['session']].drop_duplicates('session').copy()\n",
    "    tmp1 = pd.merge(tmp1, most_common, how='cross')\n",
    "    if event_name=='clicks':\n",
    "        tmp1['type'] = -1\n",
    "        tmp1['repetitions'] = -1\n",
    "        tmp1['clicks_similarity_wgt'] = -1\n",
    "        tmp1[f'{event_name}_final_wgt'] = -1\n",
    "    else:\n",
    "        tmp1['type'] = -1\n",
    "        tmp1['repetitions'] = -1\n",
    "        tmp1['top_buys_wgt'] = -1\n",
    "        tmp1['top_buy2buy_wgt'] = -1\n",
    "        tmp1[f'{event_name}_final_wgt'] = -1\n",
    "\n",
    "    df = pd.concat([df, tmp1]).sort_values(['session', f'{event_name}_final_wgt'], ascending=False)\n",
    "    df = df.drop_duplicates(['session', 'aid'], ignore_index=True)\n",
    "    df = df.sort_values(['session', f'{event_name}_final_wgt'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    df['n'] = df.groupby('session').cumcount()\n",
    "    df = df.loc[df.n < keep_amount]\n",
    "    del df['n']\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0229ae6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:27:32.660628Z",
     "iopub.status.busy": "2023-01-31T12:27:32.660243Z",
     "iopub.status.idle": "2023-01-31T12:27:32.686080Z",
     "shell.execute_reply": "2023-01-31T12:27:32.685190Z"
    },
    "papermill": {
     "duration": 0.114732,
     "end_time": "2023-01-31T12:27:32.688235",
     "exception": false,
     "start_time": "2023-01-31T12:27:32.573503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_items_from_buys_matrices(df, for_event, keep_amount=cfg.keep_amount, weight_const1=1.0):\n",
    "    aids1_df = df.loc[df.type != 0]\n",
    "    # preview_df(aids1_df)\n",
    "\n",
    "    # df = df.drop_duplicates(['session', 'aid'], ignore_index=True) # other ways of duplicates handling can be done\n",
    "    df['repetitions'] = 1\n",
    "    df = cudf.merge(\n",
    "        df[['session', 'aid', 'type','ts']], \n",
    "        df.groupby(['session', 'aid', 'type']).repetitions.sum().reset_index(),\n",
    "        on=['session', 'aid', 'type'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    df = cudf.merge(\n",
    "        df, \n",
    "        df.groupby('session').ts.max().reset_index().rename(columns={'ts': 'ts_max'}), \n",
    "        on='session', how='inner'\n",
    "    )\n",
    "\n",
    "    df['ts_distance'] = df['ts_max'] - df['ts']\n",
    "    df['ts_distance'] = np.log(df['ts_distance']+1)\n",
    "    df['ts_distance'] = df['ts_distance'].max()- df['ts_distance']\n",
    "        \n",
    "    # preview_df(df)\n",
    "\n",
    "    # print('df',df.session.nunique())\n",
    "    # Events at same time in session\n",
    "    # some sessions will be lost, because inner merge and top_20_buys_df do not have all sessions\n",
    "    # But adding df sessions at function end will fix sessions nunique to be same as test_df \n",
    "    aids2_df = cudf.merge(df[['session', 'aid']], top_20_buys_df, left_on='aid', right_on='aid_x', how='inner') \n",
    "    aids2_df.wgt = aids2_df.wgt.astype('float32')\n",
    "#     aids2_df.wgt = np.log(1+aids2_df.wgt)\n",
    "    aids2_df.loc[aids2_df.wgt > 200, 'wgt'] = 200\n",
    "\n",
    "    \n",
    "    # print('aids2', aids2_df.session.nunique())\n",
    "\n",
    "    aids2_df = aids2_df.groupby(['session', 'aid_y']).wgt.sum().reset_index()\n",
    "    aids2_df = aids2_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "    aids2_df = aids2_df.rename(columns={'aid_y': 'aid', 'wgt': 'top_buys_wgt'})\n",
    "    aids2_df['top_buy2buy_wgt'] = 0\n",
    "\n",
    "    # preview_df(aids2_df,5)\n",
    "\n",
    "    if for_event == 'carts':\n",
    "        # Carts and clicks occuring together\n",
    "        aids3_df = cudf.merge(df[['session', 'aid']], top_click2cart_df, left_on='aid', right_on='aid_x', how='inner')\n",
    "\n",
    "        aids3_df = aids3_df.groupby(['session', 'aid_y']).wgt.sum().reset_index()\n",
    "        aids3_df.wgt = aids3_df.wgt.astype('float32')\n",
    "#         aids3_df.wgt = np.log(1+aids3_df.wgt)\n",
    "        aids3_df.loc[aids3_df.wgt > 200, 'wgt'] = 200\n",
    "        \n",
    "        aids3_df = aids3_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "        aids3_df = aids3_df.rename(columns={'aid_y': 'aid', 'wgt': 'top_buy2buy_wgt'})\n",
    "        aids3_df['top_buys_wgt'] = 0\n",
    "    else:\n",
    "        # Carts and orders occuring together\n",
    "        aids3_df = cudf.merge(df[['session', 'aid']], top_20_buy2buy_df, left_on='aid', right_on='aid_x', how='inner')\n",
    "\n",
    "        aids3_df = aids3_df.groupby(['session', 'aid_y']).wgt.sum().reset_index()\n",
    "        aids3_df.wgt = aids3_df.wgt.astype('float32')\n",
    "#         aids3_df.wgt = np.log(1+aids3_df.wgt)\n",
    "        aids3_df.loc[aids3_df.wgt > 200, 'wgt'] = 200\n",
    "        \n",
    "        aids3_df = aids3_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "        aids3_df = aids3_df.rename(columns={'aid_y': 'aid', 'wgt': 'top_buy2buy_wgt'})\n",
    "        aids3_df['top_buys_wgt'] = 0\n",
    "\n",
    "    # preview_df(aids3_df,5)\n",
    "\n",
    "    aids4_df = cudf.concat([aids2_df, aids3_df], ignore_index=True)\n",
    "    aids4_df.top_buys_wgt = aids4_df.top_buys_wgt.astype('float32')\n",
    "    aids4_df.top_buy2buy_wgt = aids4_df.top_buy2buy_wgt.astype('float32')\n",
    "\n",
    "    # preview_df(aids4_df,5)\n",
    "\n",
    "    aids4_df = aids4_df.groupby(['session', 'aid']).sum().reset_index()\n",
    "    if for_event == 'carts':\n",
    "        aids4_df['wgt'] = aids4_df['top_buys_wgt'] + weight_const1 * aids4_df['top_buy2buy_wgt']\n",
    "    else:\n",
    "        aids4_df['wgt'] = aids4_df['top_buys_wgt'] + weight_const1 * aids4_df['top_buy2buy_wgt']\n",
    "        \n",
    "    aids4_df = aids4_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    # preview_df(aids4_df,5)\n",
    "    max_weight_df = aids4_df.groupby('session').wgt.max().reset_index()\n",
    "    df = cudf.merge(df, max_weight_df, on='session', how='left').fillna(1)\n",
    "    \n",
    "    df = df.sort_values(['session', 'ts'], ascending=[True, True], ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    df.wgt = df.wgt + df.repetitions\n",
    "    df.wgt = df.wgt + df.ts_distance\n",
    "    if for_event=='orders':\n",
    "        type_weghts = [1, 10, 2]\n",
    "    else:\n",
    "        type_weghts = [2, 3, 1]\n",
    "    for i in range(3):\n",
    "        df.loc[df.type==i, 'wgt'] = df.loc[df.type==i, 'wgt'] + type_weghts[i]\n",
    "    df['top_buys_wgt'] = -1\n",
    "    df['top_buy2buy_wgt'] = -1\n",
    "    del df['ts'], df['ts_max'], df['ts_distance']\n",
    "\n",
    "#     aids4_df['ts'] = df.ts.max()+1\n",
    "    aids4_df['type'] = -1\n",
    "    aids4_df['repetitions'] = -1\n",
    "    # del df['type']\n",
    "    # del df['repetitions']\n",
    "    \n",
    "#     df = df.sort_values(['session', 'ts'], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    aids4_df = cudf.concat([df, aids4_df])\n",
    "\n",
    "    aids4_df = aids4_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True)\n",
    "    aids4_df = aids4_df.drop_duplicates(['session', 'aid'], keep='first', ignore_index=True)\n",
    "    aids4_df = aids4_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True)\n",
    "    aids4_df = aids4_df.reset_index(drop=True)\n",
    "#     preview_df(aids4_df,2)\n",
    "\n",
    "\n",
    "    aids4_df['n'] = aids4_df.groupby('session').cumcount()\n",
    "    aids4_df = aids4_df.loc[aids4_df.n < keep_amount]\n",
    "    del aids4_df['n']\n",
    "    # preview_df(aids4_df,5)\n",
    "\n",
    "\n",
    "    aids4_df.repetitions = aids4_df.repetitions.astype('int8')\n",
    "    aids4_df.type = aids4_df.type.astype('int8')\n",
    "    aids4_df.top_buys_wgt = aids4_df.top_buys_wgt.astype('float32')\n",
    "    aids4_df.top_buy2buy_wgt = aids4_df.top_buy2buy_wgt.astype('float32')\n",
    "\n",
    "    aids4_df.wgt = aids4_df.wgt.astype('float32')\n",
    "    aids4_df = aids4_df.rename(columns={'wgt': f'{for_event}_final_wgt'})\n",
    "\n",
    "    return aids4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "090341b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:27:32.864387Z",
     "iopub.status.busy": "2023-01-31T12:27:32.864044Z",
     "iopub.status.idle": "2023-01-31T12:27:32.880882Z",
     "shell.execute_reply": "2023-01-31T12:27:32.880020Z"
    },
    "papermill": {
     "duration": 0.13333,
     "end_time": "2023-01-31T12:27:32.882699",
     "exception": false,
     "start_time": "2023-01-31T12:27:32.749369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_items_from_click_matrix(df, keep_amount=cfg.keep_amount):\n",
    "\n",
    "    # df = df.drop_duplicates(['session', 'aid'], ignore_index=True) # other ways of duplicates handling can be done\n",
    "    df['repetitions'] = 0.5\n",
    "    df = cudf.merge(\n",
    "        df[['session', 'aid', 'type','ts']], \n",
    "        df.groupby(['session', 'aid', 'type']).repetitions.sum().reset_index(),\n",
    "        on=['session', 'aid', 'type'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    df = cudf.merge(\n",
    "        df, \n",
    "        df.groupby('session').ts.max().reset_index().rename(columns={'ts': 'ts_max'}), \n",
    "        on='session', how='inner'\n",
    "    )\n",
    "\n",
    "    df['ts_distance'] = df['ts_max'] - df['ts']\n",
    "    df['ts_distance'] = np.log(df['ts_distance']+1)\n",
    "    df['ts_distance'] = df['ts_distance'].max()- df['ts_distance']\n",
    "\n",
    "#     preview_df(df)\n",
    "\n",
    "    aids2_df = cudf.merge(df, top_20_clicks_df, left_on='aid', right_on='aid_x', how='inner')\n",
    "    # preview_df(aids2_df,5)\n",
    "\n",
    "    aids2_df = aids2_df.groupby(['session', 'aid_y']).wgt.sum().reset_index()\n",
    "    aids2_df.wgt = aids2_df.wgt.astype('float32')\n",
    "#     aids2_df.wgt = np.log(1+aids2_df.wgt)\n",
    "    aids2_df.loc[aids2_df.wgt > 100, 'wgt'] = 100\n",
    "    \n",
    "\n",
    "    aids2_df = aids2_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "    aids2_df = aids2_df.rename(columns={'aid_y': 'aid'})\n",
    "    aids2_df['clicks_similarity_wgt'] = aids2_df['wgt']\n",
    "    # preview_df(aids2_df,5)\n",
    "\n",
    "\n",
    "    max_weight_df = aids2_df.groupby('session').wgt.max().reset_index()\n",
    "    df = cudf.merge(df, max_weight_df, on='session', how='left').fillna(1)\n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    \n",
    "    df = df.sort_values(['session', 'ts'], ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    df.wgt = df.wgt + df.ts_distance\n",
    "    df.wgt = df.wgt + df.repetitions\n",
    "    type_weghts = [1, 3, 2]\n",
    "    for i in range(3):\n",
    "        df.loc[df.type==i, 'wgt'] = df.loc[df.type==i, 'wgt'] + type_weghts[i]\n",
    "\n",
    "    del df['ts'], df['ts_max'], df['ts_distance']\n",
    "    df['clicks_similarity_wgt'] = -1\n",
    "    aids2_df['type'] = -1\n",
    "    aids2_df['repetitions'] = -1\n",
    "    aids2_df = cudf.concat([df, aids2_df])\n",
    "    # preview_df(aids2_df,5)\n",
    "    \n",
    "    aids2_df = aids2_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True)\n",
    "    aids2_df = aids2_df.drop_duplicates(['session', 'aid'], ignore_index=True)\n",
    "    aids2_df = aids2_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    aids2_df['n'] = aids2_df.groupby('session').cumcount()\n",
    "    aids2_df = aids2_df.loc[aids2_df.n < keep_amount]\n",
    "    del aids2_df['n']\n",
    "#     preview_df(aids2_df,2)\n",
    "\n",
    "\n",
    "    aids2_df.type = aids2_df.type.astype('int8')\n",
    "    aids2_df.repetitions = aids2_df.repetitions.astype('int8')\n",
    "    aids2_df.clicks_similarity_wgt = aids2_df.clicks_similarity_wgt.astype('float32')\n",
    "\n",
    "    aids2_df.wgt = aids2_df.wgt.astype('float32')\n",
    "    aids2_df = aids2_df.rename(columns={'wgt': f'clicks_final_wgt'})\n",
    "\n",
    "    return aids2_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090fe64",
   "metadata": {
    "papermill": {
     "duration": 0.05735,
     "end_time": "2023-01-31T12:27:32.998172",
     "exception": false,
     "start_time": "2023-01-31T12:27:32.940822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1788ab63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:27:33.114859Z",
     "iopub.status.busy": "2023-01-31T12:27:33.114501Z",
     "iopub.status.idle": "2023-01-31T12:27:33.855330Z",
     "shell.execute_reply": "2023-01-31T12:27:33.854237Z"
    },
    "papermill": {
     "duration": 0.802291,
     "end_time": "2023-01-31T12:27:33.857836",
     "exception": false,
     "start_time": "2023-01-31T12:27:33.055545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_20_clicks_v6_0.pqt top_20_clicks_v6_1.pqt top_20_clicks_v6_2.pqt top_20_clicks_v6_3.pqt "
     ]
    }
   ],
   "source": [
    "top_20_clicks_df = load_top_20_clicks_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19fb31d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:27:33.976623Z",
     "iopub.status.busy": "2023-01-31T12:27:33.975600Z",
     "iopub.status.idle": "2023-01-31T12:28:44.676513Z",
     "shell.execute_reply": "2023-01-31T12:28:44.675089Z"
    },
    "papermill": {
     "duration": 70.762149,
     "end_time": "2023-01-31T12:28:44.678740",
     "exception": false,
     "start_time": "2023-01-31T12:27:33.916591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  \"When using a sequence of booleans for `ascending`, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions per session 37.56719364042565\n",
      "1 Predictions per session 37.58160915415029\n",
      "2 Predictions per session 37.590892505727325\n",
      "3 Predictions per session 37.5406355985429\n",
      "4 Predictions per session 37.5459053361327\n",
      "5 Predictions per session 37.56969990608981\n",
      "6 Predictions per session 37.52839736572936\n",
      "7 Predictions per session 37.57095602969237\n",
      "8 Predictions per session 37.56210933060575\n",
      "9 Predictions per session 37.53142235036549\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "uniq_test_sessions = test_df.session.unique()\n",
    "uniq_test_sessions_df = cudf.DataFrame({'session': uniq_test_sessions})\n",
    "chunks = 10\n",
    "chunk_size = int(np.ceil(len(uniq_test_sessions) / chunks))\n",
    "for i in range(chunks):\n",
    "    print(i, end=' ')\n",
    "    from_i = i*chunk_size\n",
    "    to_i = min((i+1)*chunk_size, len(uniq_test_sessions))\n",
    "    # print(mils_format(from_i),' - ', mils_format(to_i), end=' ')\n",
    "    \n",
    "    tmp = cudf.merge(cudf.DataFrame(test_df), uniq_test_sessions_df[from_i:to_i], on='session', how='inner')\n",
    "    \n",
    "    pred_df_clicks = add_items_from_click_matrix(\n",
    "        cudf.DataFrame(tmp).sort_values([\"session\", \"ts\"]), \n",
    "        keep_amount=cfg.keep_amount,\n",
    "    )\n",
    "\n",
    "    if cfg.add_most_common:\n",
    "\n",
    "        tmp_sizes = pred_df_clicks.groupby('session').size().reset_index()\n",
    "        tmp_sizes.columns=['session', 'session_size']\n",
    "\n",
    "        less_then_keep_preds = cudf.merge(pred_df_clicks, tmp_sizes.loc[tmp_sizes.session_size < cfg.keep_amount], on='session', how='inner')\n",
    "        del less_then_keep_preds['session_size']\n",
    "        less_then_keep_preds = add_most_common(less_then_keep_preds, top_clicks_df, event_name='clicks', keep_amount=cfg.keep_amount)\n",
    "\n",
    "        pred_df_clicks = cudf.merge(pred_df_clicks, tmp_sizes.loc[tmp_sizes.session_size >= cfg.keep_amount], on='session', how='inner')\n",
    "        pred_df_clicks = pred_df_clicks.to_pandas()\n",
    "        pred_df_clicks = pd.concat([pred_df_clicks, less_then_keep_preds], ignore_index=True).sort_values('session').reset_index(drop=True)\n",
    "        pred_df_clicks.to_parquet(f'pred_df_clicks_{i}.pqt')\n",
    "\n",
    "        del tmp_sizes, less_then_keep_preds\n",
    "    else:\n",
    "        pred_df_clicks = pred_df_clicks.to_pandas()\n",
    "        pred_df_clicks.to_parquet(f'pred_df_clicks_{i}.pqt')\n",
    "\n",
    "    print('Predictions per session', pred_df_clicks.groupby('session').size().mean())\n",
    "\n",
    "    pred_df_clicks = pred_df_clicks.groupby('session').aid.apply(list)\n",
    "    pred_df_clicks = pred_df_clicks.add_suffix(\"_clicks\")\n",
    "    clicks_pred_df = pred_df_clicks.reset_index()\n",
    "    clicks_pred_df.columns = [\"session_type\", \"labels\"]\n",
    "\n",
    "    clicks_pred_df.to_parquet(f'clicks_pred_df_{i}.pqt')\n",
    "\n",
    "    del pred_df_clicks, clicks_pred_df, tmp\n",
    "    _ = gc.collect()\n",
    "\n",
    "print('---' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8671a",
   "metadata": {
    "papermill": {
     "duration": 0.058756,
     "end_time": "2023-01-31T12:28:44.798175",
     "exception": false,
     "start_time": "2023-01-31T12:28:44.739419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcbe5f18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:28:44.921817Z",
     "iopub.status.busy": "2023-01-31T12:28:44.920006Z",
     "iopub.status.idle": "2023-01-31T12:28:45.883414Z",
     "shell.execute_reply": "2023-01-31T12:28:45.882544Z"
    },
    "papermill": {
     "duration": 1.025817,
     "end_time": "2023-01-31T12:28:45.885679",
     "exception": false,
     "start_time": "2023-01-31T12:28:44.859862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_15_carts_orders_v6_0.pqt top_15_carts_orders_v6_2.pqt top_15_carts_orders_v6_1.pqt top_15_carts_orders_v6_3.pqt top_15_buy2buy_v6_0.pqt "
     ]
    }
   ],
   "source": [
    "# Events at same time in session:\n",
    "\n",
    "top_20_buys_df = load_top_20_buys_df()\n",
    "top_20_buy2buy_df = load_top_20_buy2buy_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61bbe65f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:28:46.006607Z",
     "iopub.status.busy": "2023-01-31T12:28:46.006284Z",
     "iopub.status.idle": "2023-01-31T12:30:16.573019Z",
     "shell.execute_reply": "2023-01-31T12:30:16.572046Z"
    },
    "papermill": {
     "duration": 90.631593,
     "end_time": "2023-01-31T12:30:16.577357",
     "exception": false,
     "start_time": "2023-01-31T12:28:45.945764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Predictions per session 39.424001531274484\n",
      "1 Predictions per session 39.434385486389004\n",
      "2 Predictions per session 39.4403311381078\n",
      "3 Predictions per session 39.4301804630909\n",
      "4 Predictions per session 39.419892212631815\n",
      "5 Predictions per session 39.422548016820095\n",
      "6 Predictions per session 39.44074386443435\n",
      "7 Predictions per session 39.43743009073998\n",
      "8 Predictions per session 39.44344752095035\n",
      "9 Predictions per session 39.4251857346238\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "uniq_test_sessions = test_df.session.unique()\n",
    "uniq_test_sessions_df = cudf.DataFrame({'session': uniq_test_sessions})\n",
    "chunks = 10\n",
    "chunk_size = int(np.ceil(len(uniq_test_sessions) / chunks))\n",
    "for i in range(chunks):\n",
    "    print(i, end=' ')\n",
    "    from_i = i*chunk_size\n",
    "    to_i = min((i+1)*chunk_size, len(uniq_test_sessions))\n",
    "    # print(mils_format(from_i),' - ', mils_format(to_i), end=' ')\n",
    "    \n",
    "    tmp = cudf.merge(cudf.DataFrame(test_df), uniq_test_sessions_df[from_i:to_i], on='session', how='inner')\n",
    "    \n",
    "    pred_df_orders = add_items_from_buys_matrices(\n",
    "        cudf.DataFrame(tmp).sort_values([\"session\", \"ts\"]), \n",
    "        keep_amount=cfg.keep_amount,\n",
    "        weight_const1=cfg.weight_const1_orders,\n",
    "#         weight_const1=1.5,\n",
    "        for_event='orders'\n",
    "    )\n",
    "\n",
    "    if cfg.add_most_common:\n",
    "\n",
    "        tmp_sizes = pred_df_orders.groupby('session').size().reset_index()\n",
    "        tmp_sizes.columns=['session', 'session_size']\n",
    "\n",
    "        less_then_keep_preds = cudf.merge(pred_df_orders, tmp_sizes.loc[tmp_sizes.session_size < cfg.keep_amount], on='session', how='inner')\n",
    "        del less_then_keep_preds['session_size']\n",
    "        less_then_keep_preds = add_most_common(less_then_keep_preds, top_orders_df, event_name='orders', keep_amount=cfg.keep_amount)\n",
    "\n",
    "        pred_df_orders = cudf.merge(pred_df_orders, tmp_sizes.loc[tmp_sizes.session_size >= cfg.keep_amount], on='session', how='inner')\n",
    "        pred_df_orders = pred_df_orders.to_pandas()\n",
    "        pred_df_orders = pd.concat([pred_df_orders, less_then_keep_preds], ignore_index=True).sort_values('session').reset_index(drop=True)\n",
    "        pred_df_orders.to_parquet(f'pred_df_orders_{i}.pqt')\n",
    "\n",
    "        del tmp_sizes\n",
    "    else:\n",
    "        pred_df_orders = pred_df_orders.to_pandas()\n",
    "        pred_df_orders.to_parquet(f'pred_df_orders_{i}.pqt')\n",
    "\n",
    "    print('Predictions per session', pred_df_orders.groupby('session').size().mean())\n",
    "\n",
    "\n",
    "    pred_df_orders = pred_df_orders.groupby('session').aid.apply(list)\n",
    "    pred_df_orders = pred_df_orders.add_suffix(\"_orders\")\n",
    "    orders_pred_df = pred_df_orders.reset_index()\n",
    "    orders_pred_df.columns = [\"session_type\", \"labels\"]\n",
    "\n",
    "#     preview_df(orders_pred_df)\n",
    "    # print('Orders_pred len:', len_mils(orders_pred_df))\n",
    "    orders_pred_df.to_parquet(f'orders_pred_df_{i}.pqt')\n",
    "\n",
    "    del pred_df_orders, orders_pred_df, tmp\n",
    "    _ = gc.collect()\n",
    "\n",
    "print('---' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70f3f29",
   "metadata": {
    "papermill": {
     "duration": 0.060085,
     "end_time": "2023-01-31T12:30:16.767166",
     "exception": false,
     "start_time": "2023-01-31T12:30:16.707081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Carts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e4d1ae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:30:16.889349Z",
     "iopub.status.busy": "2023-01-31T12:30:16.888997Z",
     "iopub.status.idle": "2023-01-31T12:30:16.895090Z",
     "shell.execute_reply": "2023-01-31T12:30:16.894165Z"
    },
    "papermill": {
     "duration": 0.070098,
     "end_time": "2023-01-31T12:30:16.897869",
     "exception": false,
     "start_time": "2023-01-31T12:30:16.827771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52,375,804\n",
      "56,790,872\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(len_mils(top_20_buys_df))\n",
    "except:\n",
    "    top_20_buys_df = load_top_20_buys_df()\n",
    "    print(len_mils(top_20_buys_df))\n",
    "try:\n",
    "    print(len_mils(top_click2cart_df))\n",
    "except:\n",
    "    top_click2cart_df = load_top_click2cart_df()\n",
    "    print(len_mils(top_click2cart_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "861b1a34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:30:17.022584Z",
     "iopub.status.busy": "2023-01-31T12:30:17.022199Z",
     "iopub.status.idle": "2023-01-31T12:31:46.143090Z",
     "shell.execute_reply": "2023-01-31T12:31:46.141781Z"
    },
    "papermill": {
     "duration": 89.186479,
     "end_time": "2023-01-31T12:31:46.145218",
     "exception": false,
     "start_time": "2023-01-31T12:30:16.958739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Predictions per session 39.67310280474456\n",
      "1 Predictions per session 39.65627074847022\n",
      "2 Predictions per session 39.657927635317414\n",
      "3 Predictions per session 39.655881948307524\n",
      "4 Predictions per session 39.66097223966838\n",
      "5 Predictions per session 39.65311249484092\n",
      "6 Predictions per session 39.65584605906174\n",
      "7 Predictions per session 39.65559483434122\n",
      "8 Predictions per session 39.66975912334536\n",
      "9 Predictions per session 39.65500616124517\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "uniq_test_sessions = test_df.session.unique()\n",
    "uniq_test_sessions_df = cudf.DataFrame({'session': uniq_test_sessions})\n",
    "chunks = 10\n",
    "chunk_size = int(np.ceil(len(uniq_test_sessions) / chunks))\n",
    "for i in range(chunks):\n",
    "    print(i, end=' ')\n",
    "    from_i = i*chunk_size\n",
    "    to_i = min((i+1)*chunk_size, len(uniq_test_sessions))\n",
    "    \n",
    "    tmp = cudf.merge(cudf.DataFrame(test_df), uniq_test_sessions_df[from_i:to_i], on='session', how='inner')\n",
    "    \n",
    "    pred_df_carts = add_items_from_buys_matrices(\n",
    "        cudf.DataFrame(tmp).sort_values([\"session\", \"ts\"]), \n",
    "        keep_amount=cfg.keep_amount,\n",
    "        weight_const1=cfg.weight_const1_carts,\n",
    "#         weight_const1=0.1,\n",
    "        for_event='carts'\n",
    "    )\n",
    "    \n",
    "    if cfg.add_most_common:\n",
    "        tmp_sizes = pred_df_carts.groupby('session').size().reset_index()\n",
    "        tmp_sizes.columns=['session', 'session_size']\n",
    "\n",
    "        less_then_keep_preds = cudf.merge(pred_df_carts, tmp_sizes.loc[tmp_sizes.session_size < cfg.keep_amount], on='session', how='inner')\n",
    "        del less_then_keep_preds['session_size']\n",
    "        less_then_keep_preds = add_most_common(less_then_keep_preds, top_carts_df, event_name='carts', keep_amount=cfg.keep_amount)\n",
    "\n",
    "        pred_df_carts = cudf.merge(pred_df_carts, tmp_sizes.loc[tmp_sizes.session_size >= cfg.keep_amount], on='session', how='inner')\n",
    "        pred_df_carts = pred_df_carts.to_pandas()\n",
    "        pred_df_carts = pd.concat([pred_df_carts, less_then_keep_preds], ignore_index=True).sort_values('session').reset_index(drop=True)\n",
    "        pred_df_carts.to_parquet(f'pred_df_carts_{i}.pqt')\n",
    "\n",
    "        del tmp_sizes, less_then_keep_preds\n",
    "    else:\n",
    "        pred_df_carts = pred_df_carts.to_pandas()\n",
    "        pred_df_carts.to_parquet(f'pred_df_carts_{i}.pqt')\n",
    "\n",
    "    print('Predictions per session', pred_df_carts.groupby('session').size().mean())\n",
    "\n",
    "    pred_df_carts = pred_df_carts.groupby('session').aid.apply(list)\n",
    "    pred_df_carts = pred_df_carts.add_suffix(\"_carts\")\n",
    "    carts_pred_df = pred_df_carts.reset_index()\n",
    "    carts_pred_df.columns = [\"session_type\", \"labels\"]\n",
    "    \n",
    "    carts_pred_df.to_parquet(f'carts_pred_df_{i}.pqt')\n",
    "\n",
    "    del pred_df_carts, carts_pred_df, tmp\n",
    "    _ = gc.collect()\n",
    "\n",
    "print('---' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d3ad2d",
   "metadata": {
    "papermill": {
     "duration": 0.061636,
     "end_time": "2023-01-31T12:31:46.269649",
     "exception": false,
     "start_time": "2023-01-31T12:31:46.208013",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "716a9faa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:31:46.394122Z",
     "iopub.status.busy": "2023-01-31T12:31:46.392366Z",
     "iopub.status.idle": "2023-01-31T12:33:58.319847Z",
     "shell.execute_reply": "2023-01-31T12:33:58.318726Z"
    },
    "papermill": {
     "duration": 132.054755,
     "end_time": "2023-01-31T12:33:58.385226",
     "exception": false,
     "start_time": "2023-01-31T12:31:46.330471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks 10\n",
      "1,671,803\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13468503_clicks</td>\n",
       "      <td>[1354951, 1685214, 1531805, 1833893, 1208112, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_type                                             labels\n",
       "0  13468503_clicks  [1354951, 1685214, 1531805, 1833893, 1208112, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carts 10\n",
      "1,671,803\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13468503_carts</td>\n",
       "      <td>[1354951, 1685214, 1531805, 1833893, 1208112, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     session_type                                             labels\n",
       "0  13468503_carts  [1354951, 1685214, 1531805, 1833893, 1208112, ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders 10\n",
      "1,671,803\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13099779_orders</td>\n",
       "      <td>[972319, 245308, 1100749, 1090801, 128649, 161...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_type                                             labels\n",
       "0  13099779_orders  [972319, 245308, 1100749, 1090801, 128649, 161..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission: \n",
      "5,015,409\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13468503_clicks</td>\n",
       "      <td>1354951 1685214 1531805 1833893 1208112 8557 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13468504_clicks</td>\n",
       "      <td>239165 349457 1610340 1675335 245654 945619 41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13468505_clicks</td>\n",
       "      <td>26107 558556 9282 242470 1783897 1411330 15542...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13468506_clicks</td>\n",
       "      <td>1124416 1695413 1667230 990186 380647 1533580 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13468507_clicks</td>\n",
       "      <td>1305113 604767 336884 1012794 510211 1057728 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_type                                             labels\n",
       "0  13468503_clicks  1354951 1685214 1531805 1833893 1208112 8557 1...\n",
       "1  13468504_clicks  239165 349457 1610340 1675335 245654 945619 41...\n",
       "2  13468505_clicks  26107 558556 9282 242470 1783897 1411330 15542...\n",
       "3  13468506_clicks  1124416 1695413 1667230 990186 380647 1533580 ...\n",
       "4  13468507_clicks  1305113 604767 336884 1012794 510211 1057728 1..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ev_preds_df = {}\n",
    "events_list = ['clicks', 'carts', 'orders']\n",
    "for ev in events_list:\n",
    "    files_for_ev = glob.glob(f'{ev}_pred_df*')\n",
    "    print(ev, len(files_for_ev))\n",
    "    ev_preds_df[ev] = pd.DataFrame()\n",
    "    for f in files_for_ev:\n",
    "        ev_preds_df[ev] = pd.concat([ev_preds_df[ev], pd.read_parquet(f)], ignore_index=True)\n",
    "    preview_df(ev_preds_df[ev])\n",
    "\n",
    "print('Submission: ')\n",
    "pred_df = pd.concat([ev_preds_df[ev] for ev in events_list], ignore_index=True).reset_index(drop=True)\n",
    "pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\n",
    "pred_df.to_csv(\"submission.csv\", index=False)\n",
    "preview_df(pred_df,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811928ab",
   "metadata": {
    "papermill": {
     "duration": 0.062052,
     "end_time": "2023-01-31T12:33:58.517698",
     "exception": false,
     "start_time": "2023-01-31T12:33:58.455646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1379.529673,
   "end_time": "2023-01-31T12:34:01.503248",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-31T12:11:01.973575",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

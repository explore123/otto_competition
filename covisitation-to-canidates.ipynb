{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4547e96",
   "metadata": {
    "papermill": {
     "duration": 0.013382,
     "end_time": "2023-01-31T12:11:56.040795",
     "exception": false,
     "start_time": "2023-01-31T12:11:56.027413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# About\n",
    "\n",
    "## What is done in notebook:\n",
    "\n",
    "1. Generate covisitation matrices for train set. Covisitation matrices contain information about what items are \"clicked\" together. Code for covisitation matrices is forked and modified (additional matrix is added for carts and constants used in matrices generatation are adjusted) from Chris Deotte's covistiation matrices from [here](https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575). \n",
    "\n",
    "2. Generating 40 candidates for each session from train set, using covisitation matrices. While generating candidadates, \"weights of similarity\" between items are also saved, to be used as features in further models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d476622",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7cabe97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:11:56.067762Z",
     "iopub.status.busy": "2023-01-31T12:11:56.066894Z",
     "iopub.status.idle": "2023-01-31T12:11:58.607981Z",
     "shell.execute_reply": "2023-01-31T12:11:58.606600Z"
    },
    "papermill": {
     "duration": 2.558024,
     "end_time": "2023-01-31T12:11:58.610760",
     "exception": false,
     "start_time": "2023-01-31T12:11:56.052736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use RAPIDS version 21.10.01\n"
     ]
    }
   ],
   "source": [
    "VER = 1\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import cudf, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad46bf58",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4fa644d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:00.670759Z",
     "iopub.status.busy": "2023-01-31T12:12:00.670430Z",
     "iopub.status.idle": "2023-01-31T12:12:00.676075Z",
     "shell.execute_reply": "2023-01-31T12:12:00.675006Z"
    },
    "papermill": {
     "duration": 0.021361,
     "end_time": "2023-01-31T12:12:00.678525",
     "exception": false,
     "start_time": "2023-01-31T12:12:00.657164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    local = False\n",
    "    load_in_cache = False\n",
    "\n",
    "    print_steps = True\n",
    "    \n",
    "    keep_amount = 40\n",
    "    clip_amount_in_score_calc = 40 \n",
    "    \n",
    "    weight_const1_carts = 0.1\n",
    "    weight_const1_orders = 1.5\n",
    "\n",
    "    create_submission = False\n",
    "    run_final_score_calc = False\n",
    "\n",
    "    add_most_common = False # very low (0.0001) score improvement\n",
    "    \n",
    "    recreate_matrices = False\n",
    "    save_matrices = True\n",
    "    mat_keep_amount = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg.local:\n",
    "    import rmm\n",
    "    rmm.reinitialize(pool_allocator=True, initial_pool_size=15*1024**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed17fb",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503948c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:00.703378Z",
     "iopub.status.busy": "2023-01-31T12:12:00.703116Z",
     "iopub.status.idle": "2023-01-31T12:12:00.710118Z",
     "shell.execute_reply": "2023-01-31T12:12:00.709280Z"
    },
    "papermill": {
     "duration": 0.021806,
     "end_time": "2023-01-31T12:12:00.712215",
     "exception": false,
     "start_time": "2023-01-31T12:12:00.690409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "\n",
    "def read_file(f):\n",
    "    if cfg.load_in_cache:\n",
    "        return cudf.DataFrame( data_cache[f] )\n",
    "    else:\n",
    "        return read_file_to_cudf(f)      \n",
    "def read_file_to_cudf(f):\n",
    "    df = cudf.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32')\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df\n",
    "def read_file_to_cache(f):\n",
    "    df = pd.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32')\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce2afd83",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:00.737173Z",
     "iopub.status.busy": "2023-01-31T12:12:00.736907Z",
     "iopub.status.idle": "2023-01-31T12:12:00.770244Z",
     "shell.execute_reply": "2023-01-31T12:12:00.768764Z"
    },
    "papermill": {
     "duration": 0.04845,
     "end_time": "2023-01-31T12:12:00.772463",
     "exception": false,
     "start_time": "2023-01-31T12:12:00.724013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will process 120 files, in groups of 5 and chunks of 20.\n",
      "CPU times: user 1.93 ms, sys: 647 µs, total: 2.58 ms\n",
      "Wall time: 27 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n",
    "data_cache = {}\n",
    "\n",
    "if cfg.local:\n",
    "    files = glob.glob('../input/otto-validation/*_parquet/*')\n",
    "    test_labels_file = '../../../downloaded_data/split_data_local_validation/test_labels.parquet'\n",
    "\n",
    "else:\n",
    "    files = glob.glob('../input/otto-validation/*_parquet/*')\n",
    "    test_files = glob.glob('../input/otto-validation/test_parquet/*')\n",
    "    test_labels_file = '../input/otto-validation/test_labels.parquet'\n",
    "\n",
    "    \n",
    "if cfg.load_in_cache:\n",
    "    for f in files: data_cache[f] = read_file_to_cache(f)\n",
    "\n",
    "# CHUNK PARAMETERS\n",
    "READ_CT = 5\n",
    "DIVIDE_BY = 6\n",
    "CHUNK = int( np.ceil( len(files)/DIVIDE_BY ))\n",
    "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be10738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:00.797597Z",
     "iopub.status.busy": "2023-01-31T12:12:00.797342Z",
     "iopub.status.idle": "2023-01-31T12:12:00.822010Z",
     "shell.execute_reply": "2023-01-31T12:12:00.820846Z"
    },
    "papermill": {
     "duration": 0.040059,
     "end_time": "2023-01-31T12:12:00.824641",
     "exception": false,
     "start_time": "2023-01-31T12:12:00.784582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "if cfg.recreate_matrices:\n",
    "    files_for_top_20_clicks = []\n",
    "    files_for_top_20_buys = []\n",
    "    files_for_top_20_buy2buy = []\n",
    "    files_for_click2cart = []\n",
    "else:\n",
    "    if cfg.local:\n",
    "        files_for_top_20_clicks = glob.glob('top_20_clicks_v*')\n",
    "        files_for_top_20_buys = glob.glob('top_15_carts_orders_v*')\n",
    "        files_for_top_20_buy2buy = glob.glob(f'top_15_buy2buy_v{VER}*')\n",
    "        files_for_click2cart = glob.glob('top_click2cart_v*')\n",
    "    else:\n",
    "        files_for_top_20_clicks = glob.glob('/kaggle/input/covisitation-to-canidates-dataset/top_20_clicks_*')\n",
    "        files_for_top_20_buys = glob.glob('/kaggle/input/covisitation-to-canidates-dataset/top_15_carts_orders_*')\n",
    "        files_for_top_20_buy2buy = glob.glob('/kaggle/input/covisitation-to-canidates-dataset/top_15_buy2buy_*')\n",
    "        files_for_click2cart = glob.glob('/kaggle/input/covisitation-to-canidates-dataset/top_click2cart_v*')\n",
    "\n",
    "\n",
    "print(len(files_for_top_20_clicks))\n",
    "print(len(files_for_top_20_buys))\n",
    "print(len(files_for_top_20_buy2buy))\n",
    "print(len(files_for_click2cart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "face81ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:00.850544Z",
     "iopub.status.busy": "2023-01-31T12:12:00.850290Z",
     "iopub.status.idle": "2023-01-31T12:12:00.854240Z",
     "shell.execute_reply": "2023-01-31T12:12:00.853257Z"
    },
    "papermill": {
     "duration": 0.019058,
     "end_time": "2023-01-31T12:12:00.856452",
     "exception": false,
     "start_time": "2023-01-31T12:12:00.837394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6689e283",
   "metadata": {
    "papermill": {
     "duration": 0.011966,
     "end_time": "2023-01-31T12:12:00.880466",
     "exception": false,
     "start_time": "2023-01-31T12:12:00.868500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51e7bdbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:00.905865Z",
     "iopub.status.busy": "2023-01-31T12:12:00.905581Z",
     "iopub.status.idle": "2023-01-31T12:12:00.911528Z",
     "shell.execute_reply": "2023-01-31T12:12:00.910661Z"
    },
    "papermill": {
     "duration": 0.021335,
     "end_time": "2023-01-31T12:12:00.913905",
     "exception": false,
     "start_time": "2023-01-31T12:12:00.892570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free - 447 MiB \tTotal - 16280 MiB\n"
     ]
    }
   ],
   "source": [
    "dev0 = cudf.cupy.cuda.Device(0)\n",
    "def print_gpu_mem_info():\n",
    "    free_memory, total_memory = dev0.mem_info\n",
    "    print('Free -', free_memory // 1024**2, 'MiB', '\\tTotal -', total_memory // 1024**2, 'MiB')\n",
    "def print_gpu_mem():\n",
    "    print_gpu_mem_info()\n",
    "print_gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b551dbd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:00.940227Z",
     "iopub.status.busy": "2023-01-31T12:12:00.939965Z",
     "iopub.status.idle": "2023-01-31T12:12:00.947156Z",
     "shell.execute_reply": "2023-01-31T12:12:00.946292Z"
    },
    "papermill": {
     "duration": 0.022705,
     "end_time": "2023-01-31T12:12:00.949140",
     "exception": false,
     "start_time": "2023-01-31T12:12:00.926435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ts_format(ts):\n",
    "    return datetime.datetime.fromtimestamp(ts).strftime(\"%b %d %Y  %H:%M:%S\")\n",
    "def print_date_ts(ts, end='\\n'):\n",
    "    print(ts_format(ts), end=end)\n",
    "def print_date(d, end='\\n'):\n",
    "    print(d.strftime(\"%b %d %Y  %H:%M:%S\"), end=end)\n",
    "def mils_format(x):\n",
    "    return f'{x:,}'\n",
    "def len_mils(x):\n",
    "    return mils_format(len(x))\n",
    "def preview_df(df, head_show=1):\n",
    "    print(len_mils(df))\n",
    "    display(df.head(head_show))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf7714a",
   "metadata": {
    "papermill": {
     "duration": 0.011882,
     "end_time": "2023-01-31T12:12:00.645291",
     "exception": false,
     "start_time": "2023-01-31T12:12:00.633409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Compute Co-visitation Matrices\n",
    "\n",
    "We will compute 4 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! For maximum speed, set the variable `DISK_PIECES` to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use `DISK_PIECES = 1` and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use `DISK_PIECES = 4` and it takes an amazing 3 minutes each! Below are some of the tricks to speed up computation\n",
    "* Use RAPIDS cuDF GPU instead of Pandas CPU\n",
    "* Read disk once and save in CPU RAM for later GPU multiple use\n",
    "* Process largest amount of data possible on GPU at one time\n",
    "* Merge data in two stages. Multiple small to single medium. Multiple medium to single large.\n",
    "* Write result as parquet instead of dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6f214b",
   "metadata": {
    "papermill": {
     "duration": 0.01201,
     "end_time": "2023-01-31T12:12:01.005456",
     "exception": false,
     "start_time": "2023-01-31T12:12:00.993446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95de0d21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:01.032604Z",
     "iopub.status.busy": "2023-01-31T12:12:01.032351Z",
     "iopub.status.idle": "2023-01-31T12:12:01.044470Z",
     "shell.execute_reply": "2023-01-31T12:12:01.043470Z"
    },
    "papermill": {
     "duration": 0.030285,
     "end_time": "2023-01-31T12:12:01.048050",
     "exception": false,
     "start_time": "2023-01-31T12:12:01.017765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/kaggle/input/covisitation-to-canidates-dataset/top_15_carts_orders_v6_0.pqt', '/kaggle/input/covisitation-to-canidates-dataset/top_15_carts_orders_v6_1.pqt', '/kaggle/input/covisitation-to-canidates-dataset/top_15_carts_orders_v6_2.pqt', '/kaggle/input/covisitation-to-canidates-dataset/top_15_carts_orders_v6_3.pqt']\n",
      "CPU times: user 62 µs, sys: 3 µs, total: 65 µs\n",
      "Wall time: 70.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# files_for_top_20_buys = []\n",
    "if len(files_for_top_20_buys) == 0:\n",
    "    type_weight = {0:1, 1:6, 2:3}\n",
    "\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for PART in range(DISK_PIECES):\n",
    "        print()\n",
    "        print('### DISK PART',PART+1)\n",
    "\n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(DIVIDE_BY):\n",
    "            a = j*CHUNK\n",
    "            b = min( (j+1)*CHUNK, len(files) )\n",
    "#             print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...', end=' ')\n",
    "            print('|', end=' ')\n",
    "\n",
    "            # => INNER CHUNKS\n",
    "            for k in range(a,b,READ_CT):\n",
    "                print(k, end=' ')\n",
    "                # READ FILE\n",
    "                df = [read_file(files[k])]\n",
    "                for i in range(1,READ_CT): \n",
    "                    if k+i<b: \n",
    "                        print(k+i, end=' ')\n",
    "                        df.append( read_file(files[k+i]) )\n",
    "                df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "                df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df,on='session')\n",
    "                df = df.loc[ ((df.ts_x - df.ts_y).abs()< 30 * 60) & (df.aid_x != df.aid_y) ]\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df['wgt'] = df.type_y.map(type_weight)\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a: tmp2 = df\n",
    "                else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "#                 print(k,', ',end='')\n",
    "#             print()\n",
    "            # COMBINE OUTER CHUNKS\n",
    "            if a==0: tmp = tmp2\n",
    "            else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 40\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n < cfg.mat_keep_amount].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "\n",
    "        tmp = tmp.to_pandas()\n",
    "        tmp.wgt = np.log(1.73+tmp.wgt)\n",
    "        tmp.to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')\n",
    "\n",
    "        files_for_top_20_buys = glob.glob('top_15_carts_orders_v*')\n",
    "\n",
    "        # tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')\n",
    "    del tmp\n",
    "print(files_for_top_20_buys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ce67f1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:01.073521Z",
     "iopub.status.busy": "2023-01-31T12:12:01.073268Z",
     "iopub.status.idle": "2023-01-31T12:12:07.742487Z",
     "shell.execute_reply": "2023-01-31T12:12:07.741489Z"
    },
    "papermill": {
     "duration": 6.684815,
     "end_time": "2023-01-31T12:12:07.744984",
     "exception": false,
     "start_time": "2023-01-31T12:12:01.060169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_15_carts_orders_v6_0.pqt\n",
      "top_15_carts_orders_v6_1.pqt\n",
      "top_15_carts_orders_v6_2.pqt\n",
      "top_15_carts_orders_v6_3.pqt\n"
     ]
    }
   ],
   "source": [
    "if cfg.save_matrices:\n",
    "    for f in files_for_top_20_buys:\n",
    "        tmp = cudf.read_parquet(f)\n",
    "        file_name = os.path.basename(f)\n",
    "        print(file_name)\n",
    "        tmp.to_parquet(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f8fa1f",
   "metadata": {
    "papermill": {
     "duration": 0.012236,
     "end_time": "2023-01-31T12:12:07.770010",
     "exception": false,
     "start_time": "2023-01-31T12:12:07.757774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2) \"Buy2Buy\" Co-visitation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1e71241",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:07.796711Z",
     "iopub.status.busy": "2023-01-31T12:12:07.796390Z",
     "iopub.status.idle": "2023-01-31T12:12:07.808786Z",
     "shell.execute_reply": "2023-01-31T12:12:07.807751Z"
    },
    "papermill": {
     "duration": 0.029707,
     "end_time": "2023-01-31T12:12:07.812395",
     "exception": false,
     "start_time": "2023-01-31T12:12:07.782688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/kaggle/input/covisitation-to-canidates-dataset/top_15_buy2buy_v6_0.pqt']\n",
      "CPU times: user 54 µs, sys: 13 µs, total: 67 µs\n",
      "Wall time: 71.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# files_for_top_20_buy2buy = []\n",
    "if len(files_for_top_20_buy2buy) == 0:\n",
    "\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    DISK_PIECES = 1\n",
    "    SIZE = 1.86e6/DISK_PIECES\n",
    "    DIVIDE_BY = 5\n",
    "    READ_CT = 5\n",
    "    CHUNK = int( np.ceil( len(files)/DIVIDE_BY ))\n",
    "    print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')\n",
    "\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for PART in range(DISK_PIECES):\n",
    "        print()\n",
    "        print('### DISK PART',PART+1)\n",
    "\n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(DIVIDE_BY):\n",
    "            a = j*CHUNK\n",
    "            b = min( (j+1)*CHUNK, len(files) )\n",
    "            print('|', end=' ')\n",
    "\n",
    "            # => INNER CHUNKS\n",
    "            for k in range(a,b,READ_CT):\n",
    "                # READ FILE\n",
    "                df = [read_file(files[k])]\n",
    "                print(k, end=' ')\n",
    "                for i in range(1,READ_CT): \n",
    "                    if k+i<b:\n",
    "                        print(k+i, end=' ')\n",
    "                        df.append( read_file(files[k+i]) )\n",
    "\n",
    "                df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "                df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
    "                df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<20].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df,on='session')\n",
    "                df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "\n",
    "                # df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                # df['wgt'] = 1 + 3*(df.ts_x - ts_min_used)/(ts_max_used-ts_min_used)\n",
    "\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df['wgt'] = 1\n",
    "\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a: tmp2 = df\n",
    "                else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "                # print(k,', ',end='')\n",
    "            print()\n",
    "            # COMBINE OUTER CHUNKS\n",
    "            if a==0: tmp = tmp2\n",
    "            else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 40\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n<cfg.mat_keep_amount].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "\n",
    "        tmp = tmp.to_pandas()\n",
    "        tmp.wgt = np.log(1.73+tmp.wgt)\n",
    "        tmp.to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')\n",
    "\n",
    "        # tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')\n",
    "    del tmp\n",
    "    files_for_top_20_buy2buy = glob.glob(f'top_15_buy2buy_v{VER}*')\n",
    "\n",
    "    # Carts and orders occuring together:\n",
    "\n",
    "    top_20_buy2buy_df = cudf.DataFrame()\n",
    "    for f in files_for_top_20_buy2buy:\n",
    "        print(f)\n",
    "        top_20_buy2buy_df = cudf.concat([top_20_buy2buy_df, cudf.read_parquet(f)])\n",
    "    preview_df(top_20_buy2buy_df)\n",
    "    \n",
    "print(files_for_top_20_buy2buy)\n",
    "\n",
    "# 1,314,327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46916ba6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:07.838378Z",
     "iopub.status.busy": "2023-01-31T12:12:07.838122Z",
     "iopub.status.idle": "2023-01-31T12:12:10.487898Z",
     "shell.execute_reply": "2023-01-31T12:12:10.486686Z"
    },
    "papermill": {
     "duration": 2.665836,
     "end_time": "2023-01-31T12:12:10.490777",
     "exception": false,
     "start_time": "2023-01-31T12:12:07.824941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_15_buy2buy_v6_0.pqt\n"
     ]
    }
   ],
   "source": [
    "if cfg.save_matrices:\n",
    "    for f in files_for_top_20_buy2buy:\n",
    "        tmp = cudf.read_parquet(f)\n",
    "        file_name = os.path.basename(f)\n",
    "        print(file_name)\n",
    "        tmp.to_parquet(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea73111",
   "metadata": {
    "papermill": {
     "duration": 0.01239,
     "end_time": "2023-01-31T12:12:10.516125",
     "exception": false,
     "start_time": "2023-01-31T12:12:10.503735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3) \"Clicks\" Co-visitation Matrix - Time Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa341faa",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:10.542851Z",
     "iopub.status.busy": "2023-01-31T12:12:10.542512Z",
     "iopub.status.idle": "2023-01-31T12:12:10.554018Z",
     "shell.execute_reply": "2023-01-31T12:12:10.552962Z"
    },
    "papermill": {
     "duration": 0.029154,
     "end_time": "2023-01-31T12:12:10.557701",
     "exception": false,
     "start_time": "2023-01-31T12:12:10.528547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/kaggle/input/covisitation-to-canidates-dataset/top_20_clicks_v6_3.pqt', '/kaggle/input/covisitation-to-canidates-dataset/top_20_clicks_v6_0.pqt', '/kaggle/input/covisitation-to-canidates-dataset/top_20_clicks_v6_2.pqt', '/kaggle/input/covisitation-to-canidates-dataset/top_20_clicks_v6_1.pqt']\n",
      "CPU times: user 62 µs, sys: 9 µs, total: 71 µs\n",
      "Wall time: 76.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# files_for_top_20_clicks = []\n",
    "if len(files_for_top_20_clicks) == 0:\n",
    "\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for PART in range(DISK_PIECES):\n",
    "        print()\n",
    "        print('### DISK PART',PART+1)\n",
    "\n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(DIVIDE_BY):\n",
    "            a = j*CHUNK\n",
    "            b = min( (j+1)*CHUNK, len(files) )\n",
    "            print('|', end=' ')\n",
    "\n",
    "            # => INNER CHUNKS\n",
    "            for k in range(a,b,READ_CT):\n",
    "                print(k, end=' ')\n",
    "                # READ FILE\n",
    "                df = [read_file(files[k])]\n",
    "                for i in range(1,READ_CT): \n",
    "                    if k+i<b: \n",
    "                        print(k+i, end=' ')\n",
    "                        df.append( read_file(files[k+i]) )\n",
    "                        \n",
    "                df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "                df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<30].drop('n',axis=1)\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df,on='session')\n",
    "                df = df.loc[ ((df.ts_x - df.ts_y).abs()< 30 * 60) & (df.aid_x != df.aid_y) ]\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "                # ASSIGN WEIGHTS\n",
    "                df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a: tmp2 = df\n",
    "                else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "#                 print(k,', ',end='')\n",
    "            print()\n",
    "            # COMBINE OUTER CHUNKS\n",
    "            if a==0: tmp = tmp2\n",
    "            else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 40\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n < cfg.mat_keep_amount].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "\n",
    "        tmp = tmp.to_pandas()\n",
    "        tmp.wgt = np.log(1+tmp.wgt)\n",
    "        tmp.to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')\n",
    "\n",
    "        # tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')\n",
    "        \n",
    "        files_for_top_20_clicks = glob.glob(f'top_20_clicks_v{VER}_*')\n",
    "    del tmp\n",
    "print(files_for_top_20_clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da7be7b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:10.583927Z",
     "iopub.status.busy": "2023-01-31T12:12:10.583651Z",
     "iopub.status.idle": "2023-01-31T12:12:17.764596Z",
     "shell.execute_reply": "2023-01-31T12:12:17.763577Z"
    },
    "papermill": {
     "duration": 7.196908,
     "end_time": "2023-01-31T12:12:17.767022",
     "exception": false,
     "start_time": "2023-01-31T12:12:10.570114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_20_clicks_v6_3.pqt\n",
      "top_20_clicks_v6_0.pqt\n",
      "top_20_clicks_v6_2.pqt\n",
      "top_20_clicks_v6_1.pqt\n"
     ]
    }
   ],
   "source": [
    "if cfg.save_matrices:\n",
    "    for f in files_for_top_20_clicks:\n",
    "        tmp = cudf.read_parquet(f)\n",
    "        file_name = os.path.basename(f)\n",
    "        print(file_name)\n",
    "        tmp.to_parquet(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8140773",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:17.796118Z",
     "iopub.status.busy": "2023-01-31T12:12:17.795815Z",
     "iopub.status.idle": "2023-01-31T12:12:17.800504Z",
     "shell.execute_reply": "2023-01-31T12:12:17.799503Z"
    },
    "papermill": {
     "duration": 0.02116,
     "end_time": "2023-01-31T12:12:17.802855",
     "exception": false,
     "start_time": "2023-01-31T12:12:17.781695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# _ = plt.hist(top_20_clicks_df.wgt.to_pandas(), log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f177b59",
   "metadata": {
    "papermill": {
     "duration": 0.012645,
     "end_time": "2023-01-31T12:12:17.828869",
     "exception": false,
     "start_time": "2023-01-31T12:12:17.816224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4) Click2Cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "195e9314",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:17.857971Z",
     "iopub.status.busy": "2023-01-31T12:12:17.857701Z",
     "iopub.status.idle": "2023-01-31T12:12:17.870595Z",
     "shell.execute_reply": "2023-01-31T12:12:17.869687Z"
    },
    "papermill": {
     "duration": 0.029861,
     "end_time": "2023-01-31T12:12:17.873740",
     "exception": false,
     "start_time": "2023-01-31T12:12:17.843879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/kaggle/input/covisitation-to-canidates-dataset/top_click2cart_v6_2.pqt', '/kaggle/input/covisitation-to-canidates-dataset/top_click2cart_v6_1.pqt', '/kaggle/input/covisitation-to-canidates-dataset/top_click2cart_v6_0.pqt', '/kaggle/input/covisitation-to-canidates-dataset/top_click2cart_v6_3.pqt']\n",
      "CPU times: user 82 µs, sys: 0 ns, total: 82 µs\n",
      "Wall time: 86.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# files_for_click2cart = []\n",
    "if len(files_for_click2cart) == 0:\n",
    "\n",
    "    # USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "    DISK_PIECES = 4\n",
    "    SIZE = 1.86e6/DISK_PIECES\n",
    "    READ_CT = 4\n",
    "    DIVIDE_BY = 5\n",
    "\n",
    "    # COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "    for PART in range(DISK_PIECES):\n",
    "        print()\n",
    "        print('### DISK PART',PART+1)\n",
    "\n",
    "        # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "        # => OUTER CHUNKS\n",
    "        for j in range(DIVIDE_BY):\n",
    "            a = j*CHUNK\n",
    "            b = min( (j+1)*CHUNK, len(files) )\n",
    "            print('|', end=' ')\n",
    "\n",
    "            # => INNER CHUNKS\n",
    "            for k in range(a,b,READ_CT):\n",
    "                # READ FILE\n",
    "                df = [read_file(files[k])]\n",
    "                print(k, end=' ')\n",
    "                for i in range(1,READ_CT): \n",
    "                    print(k+i, end=' ')\n",
    "                    if k+i<b: df.append( read_file(files[k+i]) )\n",
    "                df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "\n",
    "                df = df.loc[df['type'] < 2] # CLICKS and CARTS \n",
    "                df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "                # USE TAIL OF SESSION\n",
    "                df = df.reset_index(drop=True)\n",
    "                df['n'] = df.groupby('session').cumcount()\n",
    "                df = df.loc[df.n<20].drop('n',axis=1)\n",
    "\n",
    "                # CREATE PAIRS\n",
    "                df = df.merge(df,on='session')\n",
    "                df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
    "\n",
    "                # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "                df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "\n",
    "                # ASSIGN WEIGHTS\n",
    "\n",
    "                # df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                # df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "\n",
    "                df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "                df['wgt'] = 1\n",
    "                \n",
    "                df = df[['aid_x','aid_y','wgt']]\n",
    "                df.wgt = df.wgt.astype('float32')\n",
    "                df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "                \n",
    "                # COMBINE INNER CHUNKS\n",
    "                if k==a: tmp2 = df\n",
    "                else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "                # print(k,'-',k+i,', ',end='')\n",
    "            print()\n",
    "            # COMBINE OUTER CHUNKS\n",
    "            if a==0: tmp = tmp2\n",
    "            else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "            del tmp2, df\n",
    "            gc.collect()\n",
    "        # CONVERT MATRIX TO DICTIONARY\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "        # SAVE TOP 40\n",
    "        tmp = tmp.reset_index(drop=True)\n",
    "        tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "        tmp = tmp.loc[tmp.n < cfg.mat_keep_amount].drop('n',axis=1)\n",
    "        # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "        \n",
    "        tmp = tmp.to_pandas()\n",
    "        tmp.wgt = np.log(1.73+tmp.wgt)\n",
    "        tmp.to_parquet(f'top_click2cart_v{VER}_{PART}.pqt')\n",
    "\n",
    "        # tmp.to_pandas().to_parquet(f'top_click2cart_v{VER}_{PART}.pqt')\n",
    "    del tmp\n",
    "    files_for_click2cart = glob.glob('top_click2cart_v*')\n",
    "\n",
    "    # Carts and clicks occuring together:\n",
    "\n",
    "    top_click2cart_df = cudf.DataFrame()\n",
    "    for f in files_for_click2cart:\n",
    "        print(f)\n",
    "        top_click2cart_df = cudf.concat([top_click2cart_df, cudf.read_parquet(f)])\n",
    "    preview_df(top_click2cart_df)\n",
    "\n",
    "print(files_for_click2cart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "468abd3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:17.930746Z",
     "iopub.status.busy": "2023-01-31T12:12:17.930153Z",
     "iopub.status.idle": "2023-01-31T12:12:24.446397Z",
     "shell.execute_reply": "2023-01-31T12:12:24.445433Z"
    },
    "papermill": {
     "duration": 6.560615,
     "end_time": "2023-01-31T12:12:24.448752",
     "exception": false,
     "start_time": "2023-01-31T12:12:17.888137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_click2cart_v6_2.pqt\n",
      "top_click2cart_v6_1.pqt\n",
      "top_click2cart_v6_0.pqt\n",
      "top_click2cart_v6_3.pqt\n"
     ]
    }
   ],
   "source": [
    "if cfg.save_matrices:\n",
    "    for f in files_for_click2cart:\n",
    "        tmp = cudf.read_parquet(f)\n",
    "        file_name = os.path.basename(f)\n",
    "        print(file_name)\n",
    "        tmp.to_parquet(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3b5de8",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7778905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:24.478194Z",
     "iopub.status.busy": "2023-01-31T12:12:24.477271Z",
     "iopub.status.idle": "2023-01-31T12:12:24.595350Z",
     "shell.execute_reply": "2023-01-31T12:12:24.594203Z"
    },
    "papermill": {
     "duration": 0.13527,
     "end_time": "2023-01-31T12:12:24.597535",
     "exception": false,
     "start_time": "2023-01-31T12:12:24.462265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FREE MEMORY\n",
    "try:\n",
    "    if cfg.load_in_cache:\n",
    "        del data_cache\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b982b2b8",
   "metadata": {
    "papermill": {
     "duration": 0.013071,
     "end_time": "2023-01-31T12:12:24.624614",
     "exception": false,
     "start_time": "2023-01-31T12:12:24.611543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 2 - ReRank - choose candidates using handcrafted rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3227f9b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:24.686915Z",
     "iopub.status.busy": "2023-01-31T12:12:24.686645Z",
     "iopub.status.idle": "2023-01-31T12:12:27.047112Z",
     "shell.execute_reply": "2023-01-31T12:12:27.045878Z"
    },
    "papermill": {
     "duration": 2.377379,
     "end_time": "2023-01-31T12:12:27.049695",
     "exception": false,
     "start_time": "2023-01-31T12:12:24.672316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data has shape (7683577, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>aid</th>\n",
       "      <th>ts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12089221</td>\n",
       "      <td>700554</td>\n",
       "      <td>1661448002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12089221</td>\n",
       "      <td>619488</td>\n",
       "      <td>1661448024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12089221</td>\n",
       "      <td>579241</td>\n",
       "      <td>1661449547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12089221</td>\n",
       "      <td>619488</td>\n",
       "      <td>1661449585</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12089221</td>\n",
       "      <td>619488</td>\n",
       "      <td>1661456661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session     aid          ts  type\n",
       "0  12089221  700554  1661448002     0\n",
       "1  12089221  619488  1661448024     0\n",
       "2  12089221  579241  1661449547     0\n",
       "3  12089221  619488  1661449585     0\n",
       "4  12089221  619488  1661456661     0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_test():    \n",
    "    dfs = []\n",
    "    for e, chunk_file in enumerate(test_files):\n",
    "        chunk = pd.read_parquet(chunk_file)\n",
    "        chunk.ts = (chunk.ts/1000).astype('int32')\n",
    "        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n",
    "        dfs.append(chunk)\n",
    "    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n",
    "\n",
    "test_df = load_test()\n",
    "print('Test data has shape',test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f74e868d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:27.079855Z",
     "iopub.status.busy": "2023-01-31T12:12:27.078889Z",
     "iopub.status.idle": "2023-01-31T12:12:27.487361Z",
     "shell.execute_reply": "2023-01-31T12:12:27.486392Z"
    },
    "papermill": {
     "duration": 0.425577,
     "end_time": "2023-01-31T12:12:27.489917",
     "exception": false,
     "start_time": "2023-01-31T12:12:27.064340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TOP CLICKS AND ORDERS IN TEST\n",
    "\n",
    "top_clicks = test_df.loc[test_df['type']==0,'aid'].value_counts().index.values[:20]\n",
    "top_carts = test_df.loc[test_df['type']==1,'aid'].value_counts().index.values[:20]\n",
    "top_orders = test_df.loc[test_df['type']==2,'aid'].value_counts().index.values[:20]\n",
    "\n",
    "top_clicks_df = pd.DataFrame({'aid':top_clicks})\n",
    "top_carts_df = pd.DataFrame({'aid':top_carts})\n",
    "top_orders_df = pd.DataFrame({'aid':top_orders})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c3144f",
   "metadata": {},
   "source": [
    "## Functions that generate candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4021ac7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:27.519193Z",
     "iopub.status.busy": "2023-01-31T12:12:27.518345Z",
     "iopub.status.idle": "2023-01-31T12:12:27.526811Z",
     "shell.execute_reply": "2023-01-31T12:12:27.525882Z"
    },
    "papermill": {
     "duration": 0.024674,
     "end_time": "2023-01-31T12:12:27.528764",
     "exception": false,
     "start_time": "2023-01-31T12:12:27.504090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_top_20_clicks_df():\n",
    "    top_20_clicks_df = cudf.DataFrame()\n",
    "    files_for_top_20_clicks.sort()\n",
    "    for f in files_for_top_20_clicks:\n",
    "        print(f, end=' ')\n",
    "        top_20_clicks_df = cudf.concat([top_20_clicks_df, cudf.read_parquet(f)])\n",
    "    # preview_df(top_20_clicks_df)\n",
    "\n",
    "    return top_20_clicks_df\n",
    "def load_top_20_buys_df():\n",
    "    top_20_buys_df = cudf.DataFrame()\n",
    "    for f in files_for_top_20_buys:\n",
    "        print(f, end=' ')\n",
    "        top_20_buys_df = cudf.concat([top_20_buys_df, cudf.read_parquet(f)])\n",
    "    # print()\n",
    "    # preview_df(top_20_buys_df)\n",
    "\n",
    "    return top_20_buys_df\n",
    "def load_top_20_buy2buy_df():\n",
    "    # Carts and orders occuring together:\n",
    "    top_20_buy2buy_df = cudf.DataFrame()\n",
    "    for f in files_for_top_20_buy2buy:\n",
    "        print(f, end=' ')\n",
    "        top_20_buy2buy_df = cudf.concat([top_20_buy2buy_df, cudf.read_parquet(f)])\n",
    "    # preview_df(top_20_buy2buy_df)\n",
    "\n",
    "    return top_20_buy2buy_df\n",
    "def load_top_click2cart_df():\n",
    "    # Carts and clicks occuring together:\n",
    "    top_click2cart_df = cudf.DataFrame()\n",
    "    for f in files_for_click2cart:\n",
    "        print(f, end=' ')\n",
    "        top_click2cart_df = cudf.concat([top_click2cart_df, cudf.read_parquet(f)])\n",
    "    # preview_df(top_click2cart_df)\n",
    "    return top_click2cart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9537b13a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:27.556659Z",
     "iopub.status.busy": "2023-01-31T12:12:27.556369Z",
     "iopub.status.idle": "2023-01-31T12:12:27.567255Z",
     "shell.execute_reply": "2023-01-31T12:12:27.566391Z"
    },
    "papermill": {
     "duration": 0.02738,
     "end_time": "2023-01-31T12:12:27.569293",
     "exception": false,
     "start_time": "2023-01-31T12:12:27.541913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_event_recall(ev_preds_df, event_name, clip_amount=cfg.clip_amount_in_score_calc):\n",
    "    weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "    sub = ev_preds_df\n",
    "    sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "    sub.labels = sub.labels.apply(lambda x: x[:clip_amount])\n",
    "    \n",
    "    test_labels = pd.read_parquet(test_labels_file)\n",
    "    test_labels = test_labels.loc[test_labels['type']==event_name]\n",
    "    test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "\n",
    "    test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "    \n",
    "    test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0, clip_amount)\n",
    "    \n",
    "    recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "    # score += weights[event_name]*recall\n",
    "    print(f'{event_name} recall =',recall)\n",
    "\n",
    "    return recall\n",
    "    \n",
    "def calc_score(event_name, clip_amount):\n",
    "    ev_preds_df = {}\n",
    "    for ev in [event_name]:\n",
    "        files_for_ev = glob.glob(f'{ev}_pred_df*')\n",
    "        # print(ev, len(files_for_ev))\n",
    "        ev_preds_df[ev] = pd.DataFrame()\n",
    "        for f in files_for_ev:\n",
    "            ev_preds_df[ev] = pd.concat([ev_preds_df[ev], pd.read_parquet(f)], ignore_index=True)\n",
    "        # preview_df(ev_preds_df[ev])\n",
    "\n",
    "    return calc_event_recall(ev_preds_df[event_name], event_name, clip_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da4c0b6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:27.597199Z",
     "iopub.status.busy": "2023-01-31T12:12:27.596940Z",
     "iopub.status.idle": "2023-01-31T12:12:27.605339Z",
     "shell.execute_reply": "2023-01-31T12:12:27.604337Z"
    },
    "papermill": {
     "duration": 0.02484,
     "end_time": "2023-01-31T12:12:27.607593",
     "exception": false,
     "start_time": "2023-01-31T12:12:27.582753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_most_common(df, most_common, event_name, keep_amount=20):\n",
    "    df = df.to_pandas()\n",
    "    tmp1 = df[['session']].drop_duplicates('session').copy()\n",
    "    tmp1 = pd.merge(tmp1, most_common, how='cross')\n",
    "    if event_name=='clicks':\n",
    "        tmp1['type'] = -1\n",
    "        tmp1['repetitions'] = -1\n",
    "        tmp1['clicks_similarity_wgt'] = -1\n",
    "        tmp1[f'{event_name}_final_wgt'] = -1\n",
    "    else:\n",
    "        tmp1['type'] = -1\n",
    "        tmp1['repetitions'] = -1\n",
    "        tmp1['top_buys_wgt'] = -1\n",
    "        tmp1['top_buy2buy_wgt'] = -1\n",
    "        tmp1[f'{event_name}_final_wgt'] = -1\n",
    "\n",
    "    df = pd.concat([df, tmp1]).sort_values(['session', f'{event_name}_final_wgt'], ascending=False)\n",
    "    df = df.drop_duplicates(['session', 'aid'], ignore_index=True)\n",
    "    df = df.sort_values(['session', f'{event_name}_final_wgt'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    df['n'] = df.groupby('session').cumcount()\n",
    "    df = df.loc[df.n < keep_amount]\n",
    "    del df['n']\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b84a1948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:27.635668Z",
     "iopub.status.busy": "2023-01-31T12:12:27.635384Z",
     "iopub.status.idle": "2023-01-31T12:12:27.655006Z",
     "shell.execute_reply": "2023-01-31T12:12:27.654057Z"
    },
    "papermill": {
     "duration": 0.035935,
     "end_time": "2023-01-31T12:12:27.656911",
     "exception": false,
     "start_time": "2023-01-31T12:12:27.620976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_items_from_buys_matrices(df, for_event, keep_amount=cfg.keep_amount, weight_const1=1.0):\n",
    "    aids1_df = df.loc[df.type != 0]\n",
    "    # preview_df(aids1_df)\n",
    "\n",
    "    # df = df.drop_duplicates(['session', 'aid'], ignore_index=True) # other ways of duplicates handling can be done\n",
    "    df['repetitions'] = 1\n",
    "    df = df.groupby(['session', 'aid', 'type']).repetitions.sum().reset_index()\n",
    "    # preview_df(df)\n",
    "\n",
    "    # print('df',df.session.nunique())\n",
    "    # Events at same time in session\n",
    "    # some sessions will be lost, because inner merge and top_20_buys_df do not have all sessions\n",
    "    # But adding df sessions at function end will fix sessions nunique to be same as test_df \n",
    "    aids2_df = cudf.merge(df[['session', 'aid']], top_20_buys_df, left_on='aid', right_on='aid_x', how='inner') \n",
    "    aids2_df.wgt = aids2_df.wgt.astype('float32')\n",
    "    # print('aids2', aids2_df.session.nunique())\n",
    "\n",
    "    aids2_df = aids2_df.groupby(['session', 'aid_y']).wgt.sum().reset_index()\n",
    "    aids2_df = aids2_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "    aids2_df = aids2_df.rename(columns={'aid_y': 'aid', 'wgt': 'top_buys_wgt'})\n",
    "    aids2_df['top_buy2buy_wgt'] = 0\n",
    "\n",
    "    # preview_df(aids2_df,5)\n",
    "\n",
    "    if for_event == 'carts':\n",
    "        # Carts and clicks occuring together\n",
    "        aids3_df = cudf.merge(df[['session', 'aid']], top_click2cart_df, left_on='aid', right_on='aid_x', how='inner')\n",
    "\n",
    "        aids3_df = aids3_df.groupby(['session', 'aid_y']).wgt.sum().reset_index()\n",
    "        aids3_df.wgt = aids3_df.wgt.astype('float32')\n",
    "        aids3_df = aids3_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "        aids3_df = aids3_df.rename(columns={'aid_y': 'aid', 'wgt': 'top_buy2buy_wgt'})\n",
    "        aids3_df['top_buys_wgt'] = 0\n",
    "    else:\n",
    "        # Carts and orders occuring together\n",
    "        aids3_df = cudf.merge(df[['session', 'aid']], top_20_buy2buy_df, left_on='aid', right_on='aid_x', how='inner')\n",
    "\n",
    "        aids3_df = aids3_df.groupby(['session', 'aid_y']).wgt.sum().reset_index()\n",
    "        aids3_df.wgt = aids3_df.wgt.astype('float32')\n",
    "        aids3_df = aids3_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "        aids3_df = aids3_df.rename(columns={'aid_y': 'aid', 'wgt': 'top_buy2buy_wgt'})\n",
    "        aids3_df['top_buys_wgt'] = 0\n",
    "\n",
    "    # preview_df(aids3_df,5)\n",
    "\n",
    "    aids4_df = cudf.concat([aids2_df, aids3_df], ignore_index=True)\n",
    "    aids4_df.top_buys_wgt = aids4_df.top_buys_wgt.astype('float32')\n",
    "    aids4_df.top_buy2buy_wgt = aids4_df.top_buy2buy_wgt.astype('float32')\n",
    "\n",
    "    # preview_df(aids4_df,5)\n",
    "\n",
    "    aids4_df = aids4_df.groupby(['session', 'aid']).sum().reset_index()\n",
    "    if for_event == 'carts':\n",
    "        aids4_df['wgt'] = aids4_df['top_buys_wgt'] + weight_const1 * aids4_df['top_buy2buy_wgt']\n",
    "    else:\n",
    "        aids4_df['wgt'] = aids4_df['top_buys_wgt'] + weight_const1 * aids4_df['top_buy2buy_wgt']\n",
    "        \n",
    "    aids4_df = aids4_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    # preview_df(aids4_df,5)\n",
    "\n",
    "    max_weight_df = aids4_df.groupby('session').wgt.max().reset_index()\n",
    "    df = cudf.merge(df, max_weight_df, on='session', how='left').fillna(1)\n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    df.wgt = df.wgt * df.repetitions\n",
    "    if for_event=='orders':\n",
    "        type_weghts = [1, 3, 2]\n",
    "    else:\n",
    "        type_weghts = [2, 3, 1]\n",
    "    for i in range(3):\n",
    "        df.loc[df.type==i, 'wgt'] = df.loc[df.type==i, 'wgt'] + type_weghts[i]\n",
    "    df['top_buys_wgt'] = -1\n",
    "    df['top_buy2buy_wgt'] = -1\n",
    "\n",
    "    aids4_df['type'] = -1\n",
    "    aids4_df['repetitions'] = -1\n",
    "    # del df['type']\n",
    "    # del df['repetitions']\n",
    "\n",
    "\n",
    "    aids4_df = cudf.concat([df, aids4_df])\n",
    "    aids4_df = aids4_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True)\n",
    "    aids4_df = aids4_df.drop_duplicates(['session', 'aid'], ignore_index=True)\n",
    "    aids4_df = aids4_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "#     preview_df(aids4_df,2)\n",
    "\n",
    "\n",
    "    aids4_df['n'] = aids4_df.groupby('session').cumcount()\n",
    "    aids4_df = aids4_df.loc[aids4_df.n < keep_amount]\n",
    "    del aids4_df['n']\n",
    "    # preview_df(aids4_df,5)\n",
    "\n",
    "\n",
    "    aids4_df.repetitions = aids4_df.repetitions.astype('int8')\n",
    "    aids4_df.type = aids4_df.type.astype('int8')\n",
    "    aids4_df.top_buys_wgt = aids4_df.top_buys_wgt.astype('float32')\n",
    "    aids4_df.top_buy2buy_wgt = aids4_df.top_buy2buy_wgt.astype('float32')\n",
    "\n",
    "    aids4_df.wgt = aids4_df.wgt.astype('float32')\n",
    "    aids4_df = aids4_df.rename(columns={'wgt': f'{for_event}_final_wgt'})\n",
    "\n",
    "    return aids4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7566b50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:27.685804Z",
     "iopub.status.busy": "2023-01-31T12:12:27.685055Z",
     "iopub.status.idle": "2023-01-31T12:12:27.700576Z",
     "shell.execute_reply": "2023-01-31T12:12:27.699684Z"
    },
    "papermill": {
     "duration": 0.031988,
     "end_time": "2023-01-31T12:12:27.703012",
     "exception": false,
     "start_time": "2023-01-31T12:12:27.671024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_items_from_click_matrix(df, keep_amount=cfg.keep_amount):\n",
    "\n",
    "    # df = df.drop_duplicates(['session', 'aid'], ignore_index=True) # other ways of duplicates handling can be done\n",
    "    df['repetitions'] = 0.5\n",
    "    df = cudf.merge(\n",
    "        df[['session', 'aid', 'type','ts']], \n",
    "        df.groupby(['session', 'aid', 'type']).repetitions.sum().reset_index(),\n",
    "        on=['session', 'aid', 'type'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    df = cudf.merge(\n",
    "        df, \n",
    "        df.groupby('session').ts.max().reset_index().rename(columns={'ts': 'ts_max'}), \n",
    "        on='session', how='inner'\n",
    "    )\n",
    "\n",
    "    df['ts_distance'] = df['ts_max'] - df['ts']\n",
    "    df['ts_distance'] = np.log(df['ts_distance']+1)\n",
    "    df['ts_distance'] = df['ts_distance'].max()- df['ts_distance']\n",
    "\n",
    "#     preview_df(df)\n",
    "\n",
    "    aids2_df = cudf.merge(df, top_20_clicks_df, left_on='aid', right_on='aid_x', how='inner')\n",
    "    # preview_df(aids2_df,5)\n",
    "\n",
    "    aids2_df = aids2_df.groupby(['session', 'aid_y']).wgt.sum().reset_index()\n",
    "    aids2_df.wgt = aids2_df.wgt.astype('float32')\n",
    "#     aids2_df.wgt = np.log(1+aids2_df.wgt)\n",
    "    aids2_df.loc[aids2_df.wgt > 100, 'wgt'] = 100\n",
    "    \n",
    "\n",
    "    aids2_df = aids2_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "    aids2_df = aids2_df.rename(columns={'aid_y': 'aid'})\n",
    "    aids2_df['clicks_similarity_wgt'] = aids2_df['wgt']\n",
    "    # preview_df(aids2_df,5)\n",
    "\n",
    "\n",
    "    max_weight_df = aids2_df.groupby('session').wgt.max().reset_index()\n",
    "    df = cudf.merge(df, max_weight_df, on='session', how='left').fillna(1)\n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    \n",
    "    df = df.sort_values(['session', 'ts'], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#     print()\n",
    "#     print('df.ts_distance', df.ts_distance.min(), df.ts_distance.max(), df.ts_distance.mean())\n",
    "#     print('df.repetitions', df.repetitions.min(), df.repetitions.max(), df.repetitions.mean())\n",
    "#     print('df.wgt', df.wgt.min(), df.wgt.max(), df.wgt.mean())\n",
    "    \n",
    "    df.wgt = df.wgt + df.ts_distance\n",
    "    df.wgt = df.wgt + df.repetitions\n",
    "    type_weghts = [1, 3, 2]\n",
    "    for i in range(3):\n",
    "        df.loc[df.type==i, 'wgt'] = df.loc[df.type==i, 'wgt'] + type_weghts[i]\n",
    "\n",
    "    del df['ts'], df['ts_max'], df['ts_distance']\n",
    "    df['clicks_similarity_wgt'] = -1\n",
    "    aids2_df['type'] = -1\n",
    "    aids2_df['repetitions'] = -1\n",
    "    aids2_df = cudf.concat([df, aids2_df])\n",
    "    # preview_df(aids2_df,5)\n",
    "    \n",
    "    aids2_df = aids2_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True)\n",
    "    aids2_df = aids2_df.drop_duplicates(['session', 'aid'], ignore_index=True)\n",
    "    aids2_df = aids2_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    aids2_df['n'] = aids2_df.groupby('session').cumcount()\n",
    "    aids2_df = aids2_df.loc[aids2_df.n < keep_amount]\n",
    "    del aids2_df['n']\n",
    "#     preview_df(aids2_df,2)\n",
    "\n",
    "\n",
    "    aids2_df.type = aids2_df.type.astype('int8')\n",
    "    aids2_df.repetitions = aids2_df.repetitions.astype('int8')\n",
    "    aids2_df.clicks_similarity_wgt = aids2_df.clicks_similarity_wgt.astype('float32')\n",
    "\n",
    "    aids2_df.wgt = aids2_df.wgt.astype('float32')\n",
    "    aids2_df = aids2_df.rename(columns={'wgt': f'clicks_final_wgt'})\n",
    "\n",
    "    return aids2_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86f1e2a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:27.733347Z",
     "iopub.status.busy": "2023-01-31T12:12:27.733028Z",
     "iopub.status.idle": "2023-01-31T12:12:27.737110Z",
     "shell.execute_reply": "2023-01-31T12:12:27.736179Z"
    },
    "papermill": {
     "duration": 0.022693,
     "end_time": "2023-01-31T12:12:27.739443",
     "exception": false,
     "start_time": "2023-01-31T12:12:27.716750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_score = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046c853",
   "metadata": {
    "papermill": {
     "duration": 0.016219,
     "end_time": "2023-01-31T12:12:27.770503",
     "exception": false,
     "start_time": "2023-01-31T12:12:27.754284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13b03320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:27.799642Z",
     "iopub.status.busy": "2023-01-31T12:12:27.799257Z",
     "iopub.status.idle": "2023-01-31T12:12:28.263070Z",
     "shell.execute_reply": "2023-01-31T12:12:28.261911Z"
    },
    "papermill": {
     "duration": 0.480466,
     "end_time": "2023-01-31T12:12:28.265731",
     "exception": false,
     "start_time": "2023-01-31T12:12:27.785265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/covisitation-to-canidates-dataset/top_20_clicks_v6_0.pqt /kaggle/input/covisitation-to-canidates-dataset/top_20_clicks_v6_1.pqt /kaggle/input/covisitation-to-canidates-dataset/top_20_clicks_v6_2.pqt /kaggle/input/covisitation-to-canidates-dataset/top_20_clicks_v6_3.pqt "
     ]
    }
   ],
   "source": [
    "top_20_clicks_df = load_top_20_clicks_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf6078f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:28.295119Z",
     "iopub.status.busy": "2023-01-31T12:12:28.294822Z",
     "iopub.status.idle": "2023-01-31T12:12:28.300986Z",
     "shell.execute_reply": "2023-01-31T12:12:28.299907Z"
    },
    "papermill": {
     "duration": 0.024417,
     "end_time": "2023-01-31T12:12:28.304218",
     "exception": false,
     "start_time": "2023-01-31T12:12:28.279801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49067985"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len_mils(top_20_clicks_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0862f0d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:12:28.332469Z",
     "iopub.status.busy": "2023-01-31T12:12:28.332210Z",
     "iopub.status.idle": "2023-01-31T12:14:34.581284Z",
     "shell.execute_reply": "2023-01-31T12:14:34.579368Z"
    },
    "papermill": {
     "duration": 126.282251,
     "end_time": "2023-01-31T12:14:34.599850",
     "exception": false,
     "start_time": "2023-01-31T12:12:28.317599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  \"When using a sequence of booleans for `ascending`, \"\n",
      "/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  \"When using a sequence of booleans for `ascending`, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions per session 37.245417096921045\n",
      "1 Predictions per session 37.25388894440558\n",
      "2 Predictions per session 37.173228739882084\n",
      "3 Predictions per session 37.160654208720565\n",
      "4 Predictions per session 37.21449429843554\n",
      "5 Predictions per session 37.4111066697756\n",
      "6 Predictions per session 37.16907609118062\n",
      "7 Predictions per session 37.29300045523689\n",
      "8 Predictions per session 37.28649389871534\n",
      "9 Predictions per session 37.229362025794345\n",
      "------------------------------\n",
      "clicks recall = 0.5734790667682882\n",
      "0.5734790667682882\n"
     ]
    }
   ],
   "source": [
    "uniq_test_sessions = test_df.session.unique()\n",
    "uniq_test_sessions_df = cudf.DataFrame({'session': uniq_test_sessions})\n",
    "chunks = 10\n",
    "chunk_size = int(np.ceil(len(uniq_test_sessions) / chunks))\n",
    "for i in range(chunks):\n",
    "    print(i, end=' ')\n",
    "    from_i = i*chunk_size\n",
    "    to_i = min((i+1)*chunk_size, len(uniq_test_sessions))\n",
    "    # print(mils_format(from_i),' - ', mils_format(to_i), end=' ')\n",
    "    \n",
    "    tmp = cudf.merge(cudf.DataFrame(test_df), uniq_test_sessions_df[from_i:to_i], on='session', how='inner')\n",
    "    \n",
    "    pred_df_clicks = add_items_from_click_matrix(\n",
    "        cudf.DataFrame(tmp).sort_values([\"session\", \"ts\"]), \n",
    "        keep_amount=cfg.keep_amount,\n",
    "    )\n",
    "\n",
    "    if cfg.add_most_common:\n",
    "\n",
    "        tmp_sizes = pred_df_clicks.groupby('session').size().reset_index()\n",
    "        tmp_sizes.columns=['session', 'session_size']\n",
    "\n",
    "        less_then_keep_preds = cudf.merge(pred_df_clicks, tmp_sizes.loc[tmp_sizes.session_size < cfg.keep_amount], on='session', how='inner')\n",
    "        del less_then_keep_preds['session_size']\n",
    "        less_then_keep_preds = add_most_common(less_then_keep_preds, top_clicks_df, event_name='clicks', keep_amount=cfg.keep_amount)\n",
    "\n",
    "        pred_df_clicks = cudf.merge(pred_df_clicks, tmp_sizes.loc[tmp_sizes.session_size >= cfg.keep_amount], on='session', how='inner')\n",
    "        pred_df_clicks = pred_df_clicks.to_pandas()\n",
    "        pred_df_clicks = pd.concat([pred_df_clicks, less_then_keep_preds], ignore_index=True).sort_values('session').reset_index(drop=True)\n",
    "        pred_df_clicks.to_parquet(f'pred_df_clicks_{i}.pqt')\n",
    "\n",
    "        del tmp_sizes, less_then_keep_preds\n",
    "    else:\n",
    "        pred_df_clicks = pred_df_clicks.to_pandas()\n",
    "        pred_df_clicks.to_parquet(f'pred_df_clicks_{i}.pqt')\n",
    "\n",
    "    print('Predictions per session', pred_df_clicks.groupby('session').size().mean())\n",
    "\n",
    "    pred_df_clicks = pred_df_clicks.groupby('session').aid.apply(list)\n",
    "    pred_df_clicks = pred_df_clicks.add_suffix(\"_clicks\")\n",
    "    clicks_pred_df = pred_df_clicks.reset_index()\n",
    "    clicks_pred_df.columns = [\"session_type\", \"labels\"]\n",
    "\n",
    "    clicks_pred_df.to_parquet(f'clicks_pred_df_{i}.pqt')\n",
    "\n",
    "    del pred_df_clicks, clicks_pred_df, tmp\n",
    "    _ = gc.collect()\n",
    "\n",
    "print('---' * 10)\n",
    "recall_score['clicks'] = calc_score('clicks', clip_amount=cfg.clip_amount_in_score_calc)\n",
    "# recall_score['clicks'] = calc_score('clicks', clip_amount=20)\n",
    "\n",
    "print(recall_score['clicks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e618c88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:14:34.635338Z",
     "iopub.status.busy": "2023-01-31T12:14:34.634363Z",
     "iopub.status.idle": "2023-01-31T12:15:27.111601Z",
     "shell.execute_reply": "2023-01-31T12:15:27.110590Z"
    },
    "papermill": {
     "duration": 52.513719,
     "end_time": "2023-01-31T12:15:27.129112",
     "exception": false,
     "start_time": "2023-01-31T12:14:34.615393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks recall = 0.5195336575651625\n",
      "0.5195336575651625\n"
     ]
    }
   ],
   "source": [
    "if cfg.clip_amount_in_score_calc > 20:\n",
    "    print(calc_score('clicks', clip_amount=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96af907d",
   "metadata": {
    "papermill": {
     "duration": 0.015506,
     "end_time": "2023-01-31T12:15:27.203030",
     "exception": false,
     "start_time": "2023-01-31T12:15:27.187524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b94efdf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:15:27.234411Z",
     "iopub.status.busy": "2023-01-31T12:15:27.232959Z",
     "iopub.status.idle": "2023-01-31T12:15:29.307748Z",
     "shell.execute_reply": "2023-01-31T12:15:29.306486Z"
    },
    "papermill": {
     "duration": 2.092482,
     "end_time": "2023-01-31T12:15:29.310039",
     "exception": false,
     "start_time": "2023-01-31T12:15:27.217557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/covisitation-to-canidates-dataset/top_15_carts_orders_v6_0.pqt /kaggle/input/covisitation-to-canidates-dataset/top_15_carts_orders_v6_1.pqt /kaggle/input/covisitation-to-canidates-dataset/top_15_carts_orders_v6_2.pqt /kaggle/input/covisitation-to-canidates-dataset/top_15_carts_orders_v6_3.pqt /kaggle/input/covisitation-to-canidates-dataset/top_15_buy2buy_v6_0.pqt "
     ]
    }
   ],
   "source": [
    "# Events at same time in session:\n",
    "\n",
    "top_20_buys_df = load_top_20_buys_df()\n",
    "top_20_buy2buy_df = load_top_20_buy2buy_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de9b01fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:15:29.342743Z",
     "iopub.status.busy": "2023-01-31T12:15:29.341995Z",
     "iopub.status.idle": "2023-01-31T12:15:29.603082Z",
     "shell.execute_reply": "2023-01-31T12:15:29.602101Z"
    },
    "papermill": {
     "duration": 0.279772,
     "end_time": "2023-01-31T12:15:29.605473",
     "exception": false,
     "start_time": "2023-01-31T12:15:29.325701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_items_from_buys_matrices(df, for_event, keep_amount=cfg.keep_amount, weight_const1=1.0):\n",
    "    aids1_df = df.loc[df.type != 0]\n",
    "    # preview_df(aids1_df)\n",
    "\n",
    "    # df = df.drop_duplicates(['session', 'aid'], ignore_index=True) # other ways of duplicates handling can be done\n",
    "    df['repetitions'] = 1\n",
    "    df = cudf.merge(\n",
    "        df[['session', 'aid', 'type','ts']], \n",
    "        df.groupby(['session', 'aid', 'type']).repetitions.sum().reset_index(),\n",
    "        on=['session', 'aid', 'type'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    df = cudf.merge(\n",
    "        df, \n",
    "        df.groupby('session').ts.max().reset_index().rename(columns={'ts': 'ts_max'}), \n",
    "        on='session', how='inner'\n",
    "    )\n",
    "\n",
    "    df['ts_distance'] = df['ts_max'] - df['ts']\n",
    "    df['ts_distance'] = np.log(df['ts_distance']+1)\n",
    "    df['ts_distance'] = df['ts_distance'].max()- df['ts_distance']\n",
    "        \n",
    "    # preview_df(df)\n",
    "\n",
    "    # print('df',df.session.nunique())\n",
    "    # Events at same time in session\n",
    "    # some sessions will be lost, because inner merge and top_20_buys_df do not have all sessions\n",
    "    # But adding df sessions at function end will fix sessions nunique to be same as test_df \n",
    "    aids2_df = cudf.merge(df[['session', 'aid']], top_20_buys_df, left_on='aid', right_on='aid_x', how='inner') \n",
    "    aids2_df.wgt = aids2_df.wgt.astype('float32')\n",
    "#     aids2_df.wgt = np.log(1+aids2_df.wgt)\n",
    "    aids2_df.loc[aids2_df.wgt > 200, 'wgt'] = 200\n",
    "\n",
    "    \n",
    "    # print('aids2', aids2_df.session.nunique())\n",
    "\n",
    "    aids2_df = aids2_df.groupby(['session', 'aid_y']).wgt.sum().reset_index()\n",
    "    aids2_df = aids2_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "    aids2_df = aids2_df.rename(columns={'aid_y': 'aid', 'wgt': 'top_buys_wgt'})\n",
    "    aids2_df['top_buy2buy_wgt'] = 0\n",
    "\n",
    "    # preview_df(aids2_df,5)\n",
    "\n",
    "    if for_event == 'carts':\n",
    "        # Carts and clicks occuring together\n",
    "        aids3_df = cudf.merge(df[['session', 'aid']], top_click2cart_df, left_on='aid', right_on='aid_x', how='inner')\n",
    "\n",
    "        aids3_df = aids3_df.groupby(['session', 'aid_y']).wgt.sum().reset_index()\n",
    "        aids3_df.wgt = aids3_df.wgt.astype('float32')\n",
    "#         aids3_df.wgt = np.log(1+aids3_df.wgt)\n",
    "        aids3_df.loc[aids3_df.wgt > 200, 'wgt'] = 200\n",
    "        \n",
    "        aids3_df = aids3_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "        aids3_df = aids3_df.rename(columns={'aid_y': 'aid', 'wgt': 'top_buy2buy_wgt'})\n",
    "        aids3_df['top_buys_wgt'] = 0\n",
    "    else:\n",
    "        # Carts and orders occuring together\n",
    "        aids3_df = cudf.merge(df[['session', 'aid']], top_20_buy2buy_df, left_on='aid', right_on='aid_x', how='inner')\n",
    "\n",
    "        aids3_df = aids3_df.groupby(['session', 'aid_y']).wgt.sum().reset_index()\n",
    "        aids3_df.wgt = aids3_df.wgt.astype('float32')\n",
    "#         aids3_df.wgt = np.log(1+aids3_df.wgt)\n",
    "        aids3_df.loc[aids3_df.wgt > 200, 'wgt'] = 200\n",
    "        \n",
    "        aids3_df = aids3_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "        aids3_df = aids3_df.rename(columns={'aid_y': 'aid', 'wgt': 'top_buy2buy_wgt'})\n",
    "        aids3_df['top_buys_wgt'] = 0\n",
    "\n",
    "    # preview_df(aids3_df,5)\n",
    "\n",
    "    aids4_df = cudf.concat([aids2_df, aids3_df], ignore_index=True)\n",
    "    aids4_df.top_buys_wgt = aids4_df.top_buys_wgt.astype('float32')\n",
    "    aids4_df.top_buy2buy_wgt = aids4_df.top_buy2buy_wgt.astype('float32')\n",
    "\n",
    "    # preview_df(aids4_df,5)\n",
    "\n",
    "    aids4_df = aids4_df.groupby(['session', 'aid']).sum().reset_index()\n",
    "    if for_event == 'carts':\n",
    "        aids4_df['wgt'] = aids4_df['top_buys_wgt'] + weight_const1 * aids4_df['top_buy2buy_wgt']\n",
    "    else:\n",
    "        aids4_df['wgt'] = aids4_df['top_buys_wgt'] + weight_const1 * aids4_df['top_buy2buy_wgt']\n",
    "        \n",
    "    aids4_df = aids4_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    # preview_df(aids4_df,5)\n",
    "    max_weight_df = aids4_df.groupby('session').wgt.max().reset_index()\n",
    "    df = cudf.merge(df, max_weight_df, on='session', how='left').fillna(1)\n",
    "    \n",
    "    df = df.sort_values(['session', 'ts'], ascending=[True, True], ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    df.wgt = df.wgt + df.repetitions\n",
    "    df.wgt = df.wgt + df.ts_distance\n",
    "    if for_event=='orders':\n",
    "        type_weghts = [1, 10, 2]\n",
    "    else:\n",
    "        type_weghts = [2, 3, 1]\n",
    "    for i in range(3):\n",
    "        df.loc[df.type==i, 'wgt'] = df.loc[df.type==i, 'wgt'] + type_weghts[i]\n",
    "    df['top_buys_wgt'] = -1\n",
    "    df['top_buy2buy_wgt'] = -1\n",
    "    del df['ts'], df['ts_max'], df['ts_distance']\n",
    "\n",
    "#     aids4_df['ts'] = df.ts.max()+1\n",
    "    aids4_df['type'] = -1\n",
    "    aids4_df['repetitions'] = -1\n",
    "    # del df['type']\n",
    "    # del df['repetitions']\n",
    "    \n",
    "#     df = df.sort_values(['session', 'ts'], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    aids4_df = cudf.concat([df, aids4_df])\n",
    "\n",
    "    aids4_df = aids4_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True)\n",
    "    aids4_df = aids4_df.drop_duplicates(['session', 'aid'], keep='first', ignore_index=True)\n",
    "    aids4_df = aids4_df.sort_values(['session', 'wgt'], ascending=[True, False], ignore_index=True)\n",
    "    aids4_df = aids4_df.reset_index(drop=True)\n",
    "#     preview_df(aids4_df,2)\n",
    "\n",
    "\n",
    "    aids4_df['n'] = aids4_df.groupby('session').cumcount()\n",
    "    aids4_df = aids4_df.loc[aids4_df.n < keep_amount]\n",
    "    del aids4_df['n']\n",
    "    # preview_df(aids4_df,5)\n",
    "\n",
    "\n",
    "    aids4_df.repetitions = aids4_df.repetitions.astype('int8')\n",
    "    aids4_df.type = aids4_df.type.astype('int8')\n",
    "    aids4_df.top_buys_wgt = aids4_df.top_buys_wgt.astype('float32')\n",
    "    aids4_df.top_buy2buy_wgt = aids4_df.top_buy2buy_wgt.astype('float32')\n",
    "\n",
    "    aids4_df.wgt = aids4_df.wgt.astype('float32')\n",
    "    aids4_df = aids4_df.rename(columns={'wgt': f'{for_event}_final_wgt'})\n",
    "\n",
    "    return aids4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8756839f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:15:29.639021Z",
     "iopub.status.busy": "2023-01-31T12:15:29.638712Z",
     "iopub.status.idle": "2023-01-31T12:17:06.636651Z",
     "shell.execute_reply": "2023-01-31T12:17:06.635680Z"
    },
    "papermill": {
     "duration": 97.035153,
     "end_time": "2023-01-31T12:17:06.656537",
     "exception": false,
     "start_time": "2023-01-31T12:15:29.621384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Predictions per session 39.063566614480976\n",
      "1 Predictions per session 39.0377069384764\n",
      "2 Predictions per session 39.03462576196662\n",
      "3 Predictions per session 38.98121870246383\n",
      "4 Predictions per session 39.04541820725492\n",
      "5 Predictions per session 39.15981035497374\n",
      "6 Predictions per session 39.04358060468783\n",
      "7 Predictions per session 39.073826099508125\n",
      "8 Predictions per session 39.101118106214535\n",
      "9 Predictions per session 39.06642904334405\n",
      "------------------------------\n",
      "orders recall = 0.6724584327200813\n"
     ]
    }
   ],
   "source": [
    "uniq_test_sessions = test_df.session.unique()\n",
    "uniq_test_sessions_df = cudf.DataFrame({'session': uniq_test_sessions})\n",
    "chunks = 10\n",
    "chunk_size = int(np.ceil(len(uniq_test_sessions) / chunks))\n",
    "for i in range(chunks):\n",
    "    print(i, end=' ')\n",
    "    from_i = i*chunk_size\n",
    "    to_i = min((i+1)*chunk_size, len(uniq_test_sessions))\n",
    "    # print(mils_format(from_i),' - ', mils_format(to_i), end=' ')\n",
    "    \n",
    "    tmp = cudf.merge(cudf.DataFrame(test_df), uniq_test_sessions_df[from_i:to_i], on='session', how='inner')\n",
    "    \n",
    "    pred_df_orders = add_items_from_buys_matrices(\n",
    "        cudf.DataFrame(tmp).sort_values([\"session\", \"ts\"]), \n",
    "        keep_amount=cfg.keep_amount,\n",
    "        weight_const1=cfg.weight_const1_orders,\n",
    "#         weight_const1=1.5,\n",
    "        for_event='orders'\n",
    "    )\n",
    "\n",
    "    if cfg.add_most_common:\n",
    "\n",
    "        tmp_sizes = pred_df_orders.groupby('session').size().reset_index()\n",
    "        tmp_sizes.columns=['session', 'session_size']\n",
    "\n",
    "        less_then_keep_preds = cudf.merge(pred_df_orders, tmp_sizes.loc[tmp_sizes.session_size < cfg.keep_amount], on='session', how='inner')\n",
    "        del less_then_keep_preds['session_size']\n",
    "        less_then_keep_preds = add_most_common(less_then_keep_preds, top_orders_df, event_name='orders', keep_amount=cfg.keep_amount)\n",
    "\n",
    "        pred_df_orders = cudf.merge(pred_df_orders, tmp_sizes.loc[tmp_sizes.session_size >= cfg.keep_amount], on='session', how='inner')\n",
    "        pred_df_orders = pred_df_orders.to_pandas()\n",
    "        pred_df_orders = pd.concat([pred_df_orders, less_then_keep_preds], ignore_index=True).sort_values('session').reset_index(drop=True)\n",
    "        pred_df_orders.to_parquet(f'pred_df_orders_{i}.pqt')\n",
    "\n",
    "        del tmp_sizes\n",
    "    else:\n",
    "        pred_df_orders = pred_df_orders.to_pandas()\n",
    "        pred_df_orders.to_parquet(f'pred_df_orders_{i}.pqt')\n",
    "\n",
    "    print('Predictions per session', pred_df_orders.groupby('session').size().mean())\n",
    "\n",
    "\n",
    "    pred_df_orders = pred_df_orders.groupby('session').aid.apply(list)\n",
    "    pred_df_orders = pred_df_orders.add_suffix(\"_orders\")\n",
    "    orders_pred_df = pred_df_orders.reset_index()\n",
    "    orders_pred_df.columns = [\"session_type\", \"labels\"]\n",
    "\n",
    "#     preview_df(orders_pred_df)\n",
    "    # print('Orders_pred len:', len_mils(orders_pred_df))\n",
    "    orders_pred_df.to_parquet(f'orders_pred_df_{i}.pqt')\n",
    "\n",
    "    del pred_df_orders, orders_pred_df, tmp\n",
    "    _ = gc.collect()\n",
    "\n",
    "print('---' * 10)\n",
    "recall_score['orders'] = calc_score('orders', clip_amount=cfg.clip_amount_in_score_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72030744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:17:06.724373Z",
     "iopub.status.busy": "2023-01-31T12:17:06.721357Z",
     "iopub.status.idle": "2023-01-31T12:17:18.456493Z",
     "shell.execute_reply": "2023-01-31T12:17:18.454684Z"
    },
    "papermill": {
     "duration": 11.775175,
     "end_time": "2023-01-31T12:17:18.460372",
     "exception": false,
     "start_time": "2023-01-31T12:17:06.685197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders recall = 0.6478010105233591\n",
      "0.6478010105233591\n"
     ]
    }
   ],
   "source": [
    "if cfg.clip_amount_in_score_calc > 20:\n",
    "    print(calc_score('orders', clip_amount=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729309d6",
   "metadata": {
    "papermill": {
     "duration": 0.020362,
     "end_time": "2023-01-31T12:17:18.499838",
     "exception": false,
     "start_time": "2023-01-31T12:17:18.479476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Carts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fb6590b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:17:18.553435Z",
     "iopub.status.busy": "2023-01-31T12:17:18.552999Z",
     "iopub.status.idle": "2023-01-31T12:17:21.762136Z",
     "shell.execute_reply": "2023-01-31T12:17:21.760821Z"
    },
    "papermill": {
     "duration": 3.245686,
     "end_time": "2023-01-31T12:17:21.764952",
     "exception": false,
     "start_time": "2023-01-31T12:17:18.519266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49,067,985\n",
      "/kaggle/input/covisitation-to-canidates-dataset/top_click2cart_v6_2.pqt /kaggle/input/covisitation-to-canidates-dataset/top_click2cart_v6_1.pqt /kaggle/input/covisitation-to-canidates-dataset/top_click2cart_v6_0.pqt /kaggle/input/covisitation-to-canidates-dataset/top_click2cart_v6_3.pqt 53,503,866\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(len_mils(top_20_buys_df))\n",
    "except:\n",
    "    top_20_buys_df = load_top_20_buys_df()\n",
    "    print(len_mils(top_20_buys_df))\n",
    "try:\n",
    "    print(len_mils(top_click2cart_df))\n",
    "except:\n",
    "    top_click2cart_df = load_top_click2cart_df()\n",
    "    print(len_mils(top_click2cart_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "044f1ef9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:17:21.803371Z",
     "iopub.status.busy": "2023-01-31T12:17:21.803082Z",
     "iopub.status.idle": "2023-01-31T12:19:03.886272Z",
     "shell.execute_reply": "2023-01-31T12:19:03.885224Z"
    },
    "papermill": {
     "duration": 102.125279,
     "end_time": "2023-01-31T12:19:03.907291",
     "exception": false,
     "start_time": "2023-01-31T12:17:21.782012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Predictions per session 39.399259407303774\n",
      "1 Predictions per session 39.35649489801583\n",
      "2 Predictions per session 39.37376614147874\n",
      "3 Predictions per session 39.329169581293094\n",
      "4 Predictions per session 39.3644004752229\n",
      "5 Predictions per session 39.47136448930193\n",
      "6 Predictions per session 39.373099941152304\n",
      "7 Predictions per session 39.412555655485605\n",
      "8 Predictions per session 39.42992127732809\n",
      "9 Predictions per session 39.38752588595191\n",
      "------------------------------\n",
      "carts recall = 0.4424275647125343\n"
     ]
    }
   ],
   "source": [
    "uniq_test_sessions = test_df.session.unique()\n",
    "uniq_test_sessions_df = cudf.DataFrame({'session': uniq_test_sessions})\n",
    "chunks = 10\n",
    "chunk_size = int(np.ceil(len(uniq_test_sessions) / chunks))\n",
    "for i in range(chunks):\n",
    "    print(i, end=' ')\n",
    "    from_i = i*chunk_size\n",
    "    to_i = min((i+1)*chunk_size, len(uniq_test_sessions))\n",
    "    \n",
    "    tmp = cudf.merge(cudf.DataFrame(test_df), uniq_test_sessions_df[from_i:to_i], on='session', how='inner')\n",
    "    \n",
    "    pred_df_carts = add_items_from_buys_matrices(\n",
    "        cudf.DataFrame(tmp).sort_values([\"session\", \"ts\"]), \n",
    "        keep_amount=cfg.keep_amount,\n",
    "#         weight_const1=cfg.weight_const1_carts,\n",
    "        weight_const1=0.5,\n",
    "        for_event='carts'\n",
    "    )\n",
    "    \n",
    "    if cfg.add_most_common:\n",
    "        tmp_sizes = pred_df_carts.groupby('session').size().reset_index()\n",
    "        tmp_sizes.columns=['session', 'session_size']\n",
    "\n",
    "        less_then_keep_preds = cudf.merge(pred_df_carts, tmp_sizes.loc[tmp_sizes.session_size < cfg.keep_amount], on='session', how='inner')\n",
    "        del less_then_keep_preds['session_size']\n",
    "        less_then_keep_preds = add_most_common(less_then_keep_preds, top_carts_df, event_name='carts', keep_amount=cfg.keep_amount)\n",
    "\n",
    "        pred_df_carts = cudf.merge(pred_df_carts, tmp_sizes.loc[tmp_sizes.session_size >= cfg.keep_amount], on='session', how='inner')\n",
    "        pred_df_carts = pred_df_carts.to_pandas()\n",
    "        pred_df_carts = pd.concat([pred_df_carts, less_then_keep_preds], ignore_index=True).sort_values('session').reset_index(drop=True)\n",
    "        pred_df_carts.to_parquet(f'pred_df_carts_{i}.pqt')\n",
    "\n",
    "        del tmp_sizes, less_then_keep_preds\n",
    "    else:\n",
    "        pred_df_carts = pred_df_carts.to_pandas()\n",
    "        pred_df_carts.to_parquet(f'pred_df_carts_{i}.pqt')\n",
    "\n",
    "    print('Predictions per session', pred_df_carts.groupby('session').size().mean())\n",
    "\n",
    "    pred_df_carts = pred_df_carts.groupby('session').aid.apply(list)\n",
    "    pred_df_carts = pred_df_carts.add_suffix(\"_carts\")\n",
    "    carts_pred_df = pred_df_carts.reset_index()\n",
    "    carts_pred_df.columns = [\"session_type\", \"labels\"]\n",
    "    \n",
    "    carts_pred_df.to_parquet(f'carts_pred_df_{i}.pqt')\n",
    "\n",
    "    del pred_df_carts, carts_pred_df, tmp\n",
    "    _ = gc.collect()\n",
    "\n",
    "print('---' * 10)\n",
    "\n",
    "recall_score['carts'] = calc_score('carts', clip_amount=cfg.clip_amount_in_score_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5d95c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:19:03.952874Z",
     "iopub.status.busy": "2023-01-31T12:19:03.951989Z",
     "iopub.status.idle": "2023-01-31T12:19:20.092098Z",
     "shell.execute_reply": "2023-01-31T12:19:20.090411Z"
    },
    "papermill": {
     "duration": 16.168801,
     "end_time": "2023-01-31T12:19:20.094458",
     "exception": false,
     "start_time": "2023-01-31T12:19:03.925657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carts recall = 0.4040715928684677\n",
      "0.4040715928684677\n"
     ]
    }
   ],
   "source": [
    "if cfg.clip_amount_in_score_calc > 20:\n",
    "    print(calc_score('carts', clip_amount=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea490e",
   "metadata": {
    "papermill": {
     "duration": 0.017199,
     "end_time": "2023-01-31T12:19:20.199072",
     "exception": false,
     "start_time": "2023-01-31T12:19:20.181873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# All score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff744fe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:19:20.290765Z",
     "iopub.status.busy": "2023-01-31T12:19:20.290374Z",
     "iopub.status.idle": "2023-01-31T12:19:20.295687Z",
     "shell.execute_reply": "2023-01-31T12:19:20.294559Z"
    },
    "papermill": {
     "duration": 0.027121,
     "end_time": "2023-01-31T12:19:20.298437",
     "exception": false,
     "start_time": "2023-01-31T12:19:20.271316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clicks': 0.5734790667682882, 'orders': 0.6724584327200813, 'carts': 0.4424275647125343}\n"
     ]
    }
   ],
   "source": [
    "print(recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd932c92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:19:20.335939Z",
     "iopub.status.busy": "2023-01-31T12:19:20.334987Z",
     "iopub.status.idle": "2023-01-31T12:19:20.345515Z",
     "shell.execute_reply": "2023-01-31T12:19:20.344506Z"
    },
    "papermill": {
     "duration": 0.031409,
     "end_time": "2023-01-31T12:19:20.347539",
     "exception": false,
     "start_time": "2023-01-31T12:19:20.316130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks recall = 0.5734790667682882\n",
      "carts recall = 0.4424275647125343\n",
      "orders recall = 0.6724584327200813\n",
      "=============\n",
      "Overall Recall = 0.5935512357226379\n",
      "=============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5935512357226379"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_total_score(recall_score):\n",
    "    score = 0\n",
    "    weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "    for event_name in ['clicks','carts','orders']:\n",
    "        score += weights[event_name] * recall_score[event_name]\n",
    "        print(f'{event_name} recall =',recall_score[event_name])\n",
    "    \n",
    "    print('=============')\n",
    "    print('Overall Recall =',score)\n",
    "    print('=============')\n",
    "    \n",
    "    return score\n",
    "\n",
    "calc_total_score(recall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083e0f46",
   "metadata": {
    "papermill": {
     "duration": 0.017634,
     "end_time": "2023-01-31T12:19:20.426122",
     "exception": false,
     "start_time": "2023-01-31T12:19:20.408488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ff8310c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:19:20.463318Z",
     "iopub.status.busy": "2023-01-31T12:19:20.461729Z",
     "iopub.status.idle": "2023-01-31T12:19:20.757274Z",
     "shell.execute_reply": "2023-01-31T12:19:20.756304Z"
    },
    "papermill": {
     "duration": 0.316085,
     "end_time": "2023-01-31T12:19:20.759526",
     "exception": false,
     "start_time": "2023-01-31T12:19:20.443441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if cfg.create_submission:\n",
    "    ev_preds_df = {}\n",
    "    events_list = ['clicks', 'carts', 'orders']\n",
    "    for ev in events_list:\n",
    "        files_for_ev = glob.glob(f'{ev}_pred_df*')\n",
    "        print(ev, len(files_for_ev))\n",
    "        ev_preds_df[ev] = pd.DataFrame()\n",
    "        for f in files_for_ev:\n",
    "            ev_preds_df[ev] = pd.concat([ev_preds_df[ev], pd.read_parquet(f)], ignore_index=True)\n",
    "        preview_df(ev_preds_df[ev])\n",
    "\n",
    "    print('Submission: ')\n",
    "    pred_df = pd.concat([ev_preds_df[ev] for ev in events_list], ignore_index=True).reset_index(drop=True)\n",
    "    pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\n",
    "    pred_df.to_csv(\"submission.csv\", index=False)\n",
    "    preview_df(pred_df,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabca169",
   "metadata": {
    "papermill": {
     "duration": 0.017465,
     "end_time": "2023-01-31T12:19:20.797330",
     "exception": false,
     "start_time": "2023-01-31T12:19:20.779865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Compute Validation Score\n",
    "This code is from Radek [here][1]. It has been modified to use less memory.\n",
    "\n",
    "[1]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35b44113",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-31T12:19:21.949542Z",
     "iopub.status.busy": "2023-01-31T12:19:21.949209Z",
     "iopub.status.idle": "2023-01-31T12:19:21.960826Z",
     "shell.execute_reply": "2023-01-31T12:19:21.959217Z"
    },
    "papermill": {
     "duration": 0.033072,
     "end_time": "2023-01-31T12:19:21.963457",
     "exception": false,
     "start_time": "2023-01-31T12:19:21.930385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 µs, sys: 0 ns, total: 17 µs\n",
      "Wall time: 22.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# COMPUTE METRIC\n",
    "\n",
    "# if True:\n",
    "if cfg.run_final_score_calc and cfg.create_submission:\n",
    "    # FREE MEMORY\n",
    "    del top_clicks, top_orders, test_df\n",
    "    _ = gc.collect()\n",
    "    \n",
    "    score = 0\n",
    "    weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "    for t in ['clicks','carts','orders']:\n",
    "        sub = pred_df.loc[pred_df.session_type.str.contains(t)].copy()\n",
    "        sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "        sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "\n",
    "        test_labels = pd.read_parquet(test_labels_file)\n",
    "        test_labels = test_labels.loc[test_labels['type']==t]\n",
    "        test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "        test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "\n",
    "        test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "\n",
    "        recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "        score += weights[t]*recall\n",
    "        print(f'{t} recall =',recall)\n",
    "\n",
    "    print('=============')\n",
    "    print('Overall Recall =',score)\n",
    "    print('=============')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('rapids-22.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 456.484111,
   "end_time": "2023-01-31T12:19:24.490019",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-31T12:11:48.005908",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3a59f4e71348a5574ec1f78bcb3ed56991d85f87a26b0e4e23e1edb8fdce8737"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
